{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis of naval incidents in the USA, 2002 - 2015: <br>\n",
    "## Annex 3.1. Preprocess Casualty & Pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: [Oscar Anton](https://www.linkedin.com/in/oscanton/) <br>\n",
    "> Date: 2024 <br>\n",
    "> License: [CC BY-NC-ND 4.0 DEED](https://creativecommons.org/licenses/by-nc-nd/4.0/) <br>\n",
    "> Version: 0.9 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Geopositioning data management\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data folders\n",
    "import_data_folder = 'RawDataAllCasualtyAndPollution'\n",
    "export_data_folder = 'DataCasualtyAndPollution'\n",
    "\n",
    "# Maps data folder\n",
    "maps_folder = 'Maps'\n",
    "\n",
    "# Toggle for export data to external file\n",
    "file_export_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and creating a DataFrame for variable names\n",
    "variableNames = pd.read_csv(import_data_folder + '/' + 'VariableNames.txt',\n",
    "                            delimiter='\\t',\n",
    "                            encoding='ISO-8859-1')\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    variableNames.reset_index().to_feather(import_data_folder + '/' + 'VariableNames.feather')\n",
    "    print(f'variableNames {variableNames.shape} exported to {import_data_folder}')\n",
    "else:\n",
    "    variableNames = pd.read_feather(import_data_folder + '/' + 'VariableNames.feather')\n",
    "    print(f'variableNames {variableNames.shape} imported from {import_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data delimitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. VslEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and creating a DataFrame\n",
    "VslEvents = pd.read_csv(import_data_folder + '/' + 'MisleVslEvents.txt',\n",
    "                        delimiter='\\t',\n",
    "                        encoding='ISO-8859-1')\n",
    "\n",
    "# Dataframe variable names from variableNames\n",
    "VslEvents.columns = variableNames.query('Table == \"MisleVslEvents\"')['Name'].tolist()\n",
    "\n",
    "# timeline_dt variable set up\n",
    "VslEvents['timeline_dt'] = pd.to_datetime(VslEvents['timeline_dt'],\n",
    "                                          format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "VslEvents['date'] = VslEvents['timeline_dt']\n",
    "VslEvents['hour'] = VslEvents['timeline_dt'].dt.time\n",
    "\n",
    "# Variable preselection\n",
    "VslEvents = VslEvents.drop(['timeline_dt', 'case_id', 'fk_d_vessel',\n",
    "                            'vessel_service', 'vessel_type', 'vessel_subtype',\n",
    "                            'event_class', 'event_subclass'], axis=1)\n",
    "\n",
    "# Filter by coords and date\n",
    "VslEvents = VslEvents[\n",
    "    (VslEvents['latitude'].between(15, 70)) &\n",
    "    (VslEvents['longitude'].between(-180, -45)) &\n",
    "    (VslEvents['date'].between(pd.to_datetime('2002-01-01'),\n",
    "                               pd.to_datetime('2015-12-31')))\n",
    "]\n",
    "\n",
    "# Distinct rows by ID variables\n",
    "VslEvents = VslEvents.drop_duplicates(subset=['activity_id', 'vessel_id', 'event_type'], keep='first')\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    VslEvents.reset_index().to_feather(export_data_folder + '/' + 'VslEvents.feather')\n",
    "    print(f'VslEvents {VslEvents.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    VslEvents = pd.read_feather(export_data_folder + '/' + 'VslEvents.feather')\n",
    "    print(f'VslEvents {VslEvents.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and creating a DataFrame\n",
    "Vessel = pd.read_csv(import_data_folder + '/' + 'MisleVessel.txt',\n",
    "                     delimiter='\\t',\n",
    "                     encoding='ISO-8859-1',\n",
    "                     quoting=3,\n",
    "                     low_memory=False,\n",
    "                     usecols=range(66))\n",
    "\n",
    "# Dataframe variable names from variableNames\n",
    "Vessel.columns = variableNames.query('Table == \"MisleVessel\"')['Name'].tolist()\n",
    "\n",
    "# Variable preselection\n",
    "Vessel = Vessel.drop(['gk_d_vessel', 'managing_owner_id', 'managing_owner',\n",
    "                      'net_ton', 'itc_breadth', 'itc_depth', 'itc_gross_ton',\n",
    "                      'itc_length', 'itc_net_ton', 'draft_design', 'draft_design_units',\n",
    "                      'deadweighttonnage_units', 'hailing_port', 'hailing_port_state',\n",
    "                      'hailing_port_province', 'route_type', 'cargo_authorization_type',\n",
    "                      'documented_ind', 'documented_status_type', 'inspected_ind',\n",
    "                      'inspected_desc', 'state_vessel_ind', 'state_vessel_desc',\n",
    "                      'lloyds_ind', 'lloyds_desc', 'solas_ind', 'insp_subchapter_type',\n",
    "                      'vessel_type', 'vessel_subtype', 'vessel_service',\n",
    "                      'max_passengers_allowed', 'max_crew', 'self_propelled_ind',\n",
    "                      'call_sign', 'official_number', 'hull_number', 'rbs_hull_number',\n",
    "                      'vessel_age', 'hull_build_party_name', 'completed_by_party_name',\n",
    "                      'filler'], axis=1)\n",
    "\n",
    "# Distinct rows by ID variables\n",
    "Vessel = Vessel.drop_duplicates(\n",
    "    subset=['vessel_id', 'vessel_name'], keep='first')\n",
    "\n",
    "# Fix dtypes\n",
    "Vessel['dead_weight_ton'] = pd.to_numeric(Vessel['dead_weight_ton'], errors='coerce')\n",
    "Vessel['primary_vin'] = Vessel['primary_vin'].astype(str)\n",
    "Vessel['imo_number'] = Vessel['imo_number'].astype(str)\n",
    "Vessel['build_year'] = Vessel['build_year'].astype(str)\n",
    "Vessel['horsepower_ahead'] = pd.to_numeric(Vessel['horsepower_ahead'], errors='coerce')\n",
    "Vessel['horsepower_astern'] = pd.to_numeric(Vessel['horsepower_astern'], errors='coerce')\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    Vessel.reset_index().to_feather(export_data_folder + '/' + 'Vessel.feather')\n",
    "    print(f'Vessel {Vessel.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    Vessel = pd.read_feather(export_data_folder + '/' + 'Vessel.feather')\n",
    "    print(f'Vessel {Vessel.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and creating a DataFrame\n",
    "Injury = pd.read_csv(import_data_folder + '/' + 'MisleInjury.txt',\n",
    "                     delimiter='\\t',\n",
    "                     encoding='ISO-8859-1')\n",
    "\n",
    "# Dataframe variable names from variableNames\n",
    "Injury.columns = variableNames.query('Table == \"MisleInjury\"')['Name'].tolist()\n",
    "\n",
    "# Variable preselection\n",
    "Injury = Injury.drop(['fk_d_vessel', 'vessel_service', 'vessel_type',\n",
    "                      'vessel_subtype', 'facility_id', 'facility_name',\n",
    "                      'facility_type_desc', 'facility_activity_role_desc'],\n",
    "                     axis=1)\n",
    "\n",
    "# Filter by coords\n",
    "Injury = Injury[\n",
    "    (Injury['latitude'].between(15, 70)) &\n",
    "    (Injury['longitude'].between(-180, -45))]\n",
    "\n",
    "# Distinct rows by ID variables\n",
    "Injury = Injury.drop_duplicates(\n",
    "    subset=['activity_id', 'vessel_id'], keep='first')\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    Injury.reset_index().to_feather(export_data_folder + '/' + 'Injury.feather')\n",
    "    print(f'Injury {Injury.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    Injury = pd.read_feather(export_data_folder + '/' + 'Injury.feather')\n",
    "    print(f'Injury {Injury.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. VslPoll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and creating a DataFrame\n",
    "VslPoll = pd.read_csv(import_data_folder + '/' + 'MisleVslPoll.txt',\n",
    "                      delimiter='\\t',\n",
    "                      encoding='ISO-8859-1')\n",
    "\n",
    "# Dataframe variable names from variableNames\n",
    "VslPoll.columns = variableNames.query('Table == \"MisleVslPoll\"')['Name'].tolist()\n",
    "\n",
    "# Variable preselection\n",
    "VslPoll = VslPoll.drop(['case_id', 'fk_d_vessel', 'vessel_service', 'vessel_type',\n",
    "                        'vessel_subtype','substance_name', 'substance_class',\n",
    "                        'substance_subclass', 'substance_type', 'substance_subtype',\n",
    "                        'discharge_amnt_water', 'discharge_amnt_land', 'discharge_amnt_air',\n",
    "                        'discharge_amnt_enclosed', 'potential_amnt_total',\n",
    "                        'potential_amnt_water', 'potential_amnt_land', 'potential_amnt_air',\n",
    "                        'potential_amnt_enclosed', 'contained_amnt', 'discharge_potential_type',\n",
    "                        'discharge_situation_type', 'discharge_estimated_land',\n",
    "                        'discharge_estimated_air', 'discharge_estimated_water',\n",
    "                        'discharge_estimated_encl', 'potential_case', 'potential_estimated',\n",
    "                        'contained_estimated', 'unit_of_measure'], axis=1)\n",
    "\n",
    "# Filter by coords\n",
    "VslPoll = VslPoll[\n",
    "    (VslPoll['latitude'].between(15, 70)) &\n",
    "    (VslPoll['longitude'].between(-180, -45))]\n",
    "\n",
    "# Distinct rows by ID variables\n",
    "VslPoll = VslPoll.drop_duplicates(subset=['activity_id', 'vessel_id'], keep='first')\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    VslPoll.reset_index().to_feather(export_data_folder + '/' + 'VslPoll.feather')\n",
    "    print(f'VslPoll {VslPoll.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    VslPoll = pd.read_feather(export_data_folder + '/' + 'VslPoll.feather')\n",
    "    print(f'VslPoll {VslPoll.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file and creating a DataFrame\n",
    "Activity = pd.read_csv(import_data_folder + '/' + 'MisleActivity.txt',\n",
    "                       delimiter='\\t',\n",
    "                       encoding='ISO-8859-1')\n",
    "\n",
    "# Dataframe variable names from variableNames\n",
    "Activity.columns = variableNames.query('Table == \"MisleActivity\"')['Name'].tolist()\n",
    "\n",
    "# incident_dt variable set up\n",
    "Activity['date'] = pd.to_datetime(Activity['incident_dt'], format=\"%m/%d/%Y\")\n",
    "\n",
    "# damage_assessment variable set up\n",
    "Activity['damage_assessment'] = (\n",
    "    Activity['vessel_property_damage'] +\n",
    "    Activity['cargo_property_damage'] +\n",
    "    Activity['facility_property_damage'] +\n",
    "    Activity['other_property_damage'])\n",
    "\n",
    "# Variable preselection\n",
    "Activity = Activity.drop(['case_id', 'incident_dt', 'dept_name', 'activity_type',\n",
    "                          'activity_status', 'activity_status_subtype',\n",
    "                          'vessel_property_damage', 'cargo_property_damage',\n",
    "                          'facility_property_damage', 'other_property_damage'],\n",
    "                         axis=1)\n",
    "\n",
    "# Filter by date\n",
    "Activity = Activity[(Activity['date'].between(pd.to_datetime('2002-01-01', format=\"%Y-%m-%d\"),\n",
    "                                              pd.to_datetime('2015-12-31', format=\"%Y-%m-%d\")))]\n",
    "\n",
    "# Distinct rows by ID variables\n",
    "Activity = Activity.drop_duplicates(subset=['activity_id', 'date'], keep='first')\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    Activity.reset_index().to_feather(export_data_folder + '/' + 'Activity.feather')\n",
    "    print(f'Activity {Activity.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    Activity = pd.read_feather(export_data_folder + '/' + 'Activity.feather')\n",
    "    print(f'Activity {Activity.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Events: Geographic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe will be used for the weather data managing\n",
    "Events = VslEvents.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Region (new variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for region coords\n",
    "def assign_region(row):\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "\n",
    "    if 49 <= latitude <= 70 and -180 <= longitude <= -122:\n",
    "        return \"Alaska\"\n",
    "    elif 49 <= latitude <= 70 and -122 <= longitude <= -45:\n",
    "        return \"Canada\"\n",
    "    elif 15 <= latitude <= 49 and -81.5 <= longitude <= -45:\n",
    "        return \"East Coast\"\n",
    "    elif 15 <= latitude <= 49 and -180 <= longitude <= -100:\n",
    "        return \"West Coast\"\n",
    "    elif 15 <= latitude <= 31 and -100 <= longitude <= -81.5:\n",
    "        return \"Gulf of Mexico\"\n",
    "    elif 31 <= latitude <= 49 and -100 <= longitude <= -81.5:\n",
    "        return \"Mississippi\"\n",
    "    else:\n",
    "        return \"Other Region\"\n",
    "\n",
    "# Function apply to each row of dataframe\n",
    "Events['region'] = Events.apply(assign_region, axis=1)\n",
    "\n",
    "# Check new variable counts\n",
    "print(Events['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region colors set up\n",
    "colors = {'Alaska': 'white', 'Canada': 'red', 'East Coast': 'blue',\n",
    "          'West Coast': 'orange', 'Gulf of Mexico': 'brown', 'Mississippi': 'green',\n",
    "          'Other Region': 'black'}\n",
    "\n",
    "# Create figure object\n",
    "fig = go.Figure()\n",
    "\n",
    "# Aggregate Events points\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=Events['latitude'],\n",
    "    lon=Events['longitude'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=Events['region'].map(colors), opacity=0.5),\n",
    "    text=Events.apply(lambda row:f\"region:{row['region']}<br>activity_id: {row['activity_id']}\", axis=1)))\n",
    "\n",
    "# Set up map design\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {'style': \"open-street-map\", 'center': {'lon': -112, 'lat': 48}, 'zoom': 2})\n",
    "\n",
    "# Show map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Watertype: river / ocean (new variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://www.matecdev.com/posts/point-in-polygon.html\n",
    "\n",
    "# Load Shapefile for only EEUU and surroundings (from rnaturalearthdata library)\n",
    "gdf = gpd.read_file(maps_folder + '/' + 'continental.shp')\n",
    "\n",
    "# Create a GeoDataFrame from the point DataFrame\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(Events['longitude'], Events['latitude'])]\n",
    "gdf_points = gpd.GeoDataFrame(Events, geometry=geometry, crs=gdf.crs)\n",
    "\n",
    "# Perform spatial join between the GeoDataFrame of points and the GeoDataFrame of polygons\n",
    "result_sjoin = gpd.tools.sjoin(gdf_points, gdf, how=\"left\", predicate='within')\n",
    "\n",
    "# The 'index_right' column will contain the indices of the polygons where each point is located\n",
    "# Create a new variable in your original DataFrame: ocean or river\n",
    "Events['watertype'] = result_sjoin['index_right'].notnull().astype(int)\n",
    "Events['watertype'] = np.where(Events['watertype'] == 0, 'ocean', 'river')\n",
    "\n",
    "# Check new variable counts\n",
    "Events['watertype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watertype visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure object\n",
    "fig = go.Figure()\n",
    "\n",
    "# Aggregate Events points\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=Events['latitude'],\n",
    "    lon=Events['longitude'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5,\n",
    "                color=Events['watertype'].map({'ocean': 'blue', 'river': 'green'}),\n",
    "                opacity=0.5),\n",
    "    text=Events.apply(lambda row:f\"watertype:{row['watertype']}<br>activity_id: {row['activity_id']}\", axis=1)))\n",
    "\n",
    "# Set up map design\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {'style': \"open-street-map\", 'center': {'lon': -112, 'lat': 48}, 'zoom': 2})\n",
    "\n",
    "# Show map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Watertype delimitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter application: only north-american oceans and Mississippi river\n",
    "Events = Events[(Events['watertype'] != 'river') | (Events['region'] == 'Mississippi')]\n",
    "Events = Events.dropna(subset=['vessel_id'])\n",
    "\n",
    "# Print first observations\n",
    "Events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Dataframe export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    Events.reset_index().to_feather(export_data_folder + '/' + 'Events.feather')\n",
    "    print(f'Events {Events.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    print('Events {Events.shape} already exported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #2fa4e7;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
