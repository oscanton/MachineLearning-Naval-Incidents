{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis of naval incidents in the USA, 2002 - 2015: <br>\n",
    "## Annex 3.2. Preprocess Weather Ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: [Oscar Anton](https://www.linkedin.com/in/oscanton/) <br>\n",
    "> Date: 2024 <br>\n",
    "> License: [CC BY-NC-ND 4.0 DEED](https://creativecommons.org/licenses/by-nc-nd/4.0/) <br>\n",
    "> Version: 0.9 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File management\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data folders\n",
    "import_data_folder= 'RawDataWeatherOcean'\n",
    "export_data_folder= 'DataWeatherOcean'\n",
    "\n",
    "# Toggle for export data to external file\n",
    "file_export_enabled = False\n",
    "# Toggle for calculations that takes a long time\n",
    "protracted_calculation_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Compile monthly maritime meteorology data from the NOAA website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if protracted_calculation_enabled :\n",
    "    # Initiate list to store DataFrames from each .csv file\n",
    "    filtered_dataframes = []\n",
    "\n",
    "    # Columns to select from each CSV file\n",
    "    selected_columns = {\n",
    "        'STATION': str,\n",
    "        'DATE': str,\n",
    "        'LATITUDE': 'float32',\n",
    "        'LONGITUDE': 'float32',\n",
    "        'PAST_WX': 'float32',\n",
    "        'WIND_SPEED': 'float32',\n",
    "        'VISIBILITY': 'float32',\n",
    "        'AIR_TEMP': 'float32',\n",
    "        'WAVE_HGT': 'float32'\n",
    "    }\n",
    "\n",
    "    # Iterate over the .tar.gz files in the folder\n",
    "    for tar_file in os.listdir(import_data_folder):\n",
    "        if tar_file.endswith('.tar.gz'):\n",
    "            tar_file_path = os.path.join(import_data_folder, tar_file)\n",
    "    \n",
    "            # Extract the .tar.gz file\n",
    "            with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
    "                # Find .csv files within the .tar.gz\n",
    "                csv_files = [member for member in tar.getmembers() if member.name.endswith('.csv')]\n",
    "    \n",
    "                # Read each .csv file and store in a DataFrame\n",
    "                for csv_file in csv_files:\n",
    "                    with tar.extractfile(csv_file) as file:\n",
    "                        # Read the CSV file and handle missing columns\n",
    "                        df = pd.read_csv(file, index_col=False, dtype=selected_columns).\\\n",
    "                            reindex(columns=selected_columns.keys())\n",
    "    \n",
    "                        # Apply filter for NAs\n",
    "                        df_filtered = df.dropna(subset=['STATION', 'LONGITUDE', 'LATITUDE']).\\\n",
    "                            dropna(thresh=len(df.columns) - 4)\n",
    "                        \n",
    "                        # Apply filter for bounding box\n",
    "                        df_filtered = df_filtered[df_filtered['LONGITUDE'].between(-180, -45) &\n",
    "                              df_filtered['LATITUDE'].between(15, 70)]\n",
    "    \n",
    "                        filtered_dataframes.append(df_filtered)\n",
    "    \n",
    "    # Concatenate all DataFrames into merged one\n",
    "    marine_stations_comb_1 = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "    # Column names to lowercase\n",
    "    marine_stations_comb_1.columns = marine_stations_comb_1.columns.str.lower()\n",
    "    print(f'marine_stations_comb_1 {marine_stations_comb_1.shape} created')\n",
    "else:\n",
    "    marine_stations_comb_1 = pd.read_feather(export_data_folder + '/' + 'marine_stations_comb_1.feather')\n",
    "    print(f'marine_stations_comb_1 {marine_stations_comb_1.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or export to external file\n",
    "if file_export_enabled :\n",
    "    marine_stations_comb_1.to_feather(export_data_folder + '/' + 'marine_stations_comb_1.feather')\n",
    "    print(f'marine_stations_comb_1 {marine_stations_comb_1.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    marine_stations_comb_1 = pd.read_feather(export_data_folder + '/' + 'marine_stations_comb_1.feather')\n",
    "    print(f'marine_stations_comb_1 {marine_stations_comb_1.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Daily means for Stations' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only date, leaving hour\n",
    "marine_stations_comb_1['date'] = pd.to_datetime(marine_stations_comb_1['date']).dt.date\n",
    "\n",
    "# Select values to summarize\n",
    "values = list(['latitude', 'longitude', 'past_wx',\n",
    "               'wind_speed', 'visibility', 'air_temp', 'wave_hgt'])\n",
    "\n",
    "# Calculate the mean according to STATION and DATE\n",
    "marine_stations_daily_2 = (marine_stations_comb_1\n",
    "                           .groupby(['station', 'date'])[values]\n",
    "                           .mean()\n",
    "                           .reset_index())\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    marine_stations_daily_2.to_feather(export_data_folder + '/' + 'marine_stations_daily_2.feather')\n",
    "    print(f'marine_stations_daily_2 {marine_stations_daily_2.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    marine_stations_daily_2 = pd.read_feather(export_data_folder + '/' + 'marine_stations_daily_2.feather')\n",
    "    print(f'marine_stations_daily_2 {marine_stations_daily_2.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Join activity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Load ocean events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "Events = pd.read_feather('DataCasualtyAndPollution' + '/' + 'Events.feather')\n",
    "\n",
    "# Variable selection\n",
    "EventsOcean = Events[(Events.watertype == 'ocean')][['activity_id', 'date', 'longitude', 'latitude']]\n",
    "\n",
    "# Extract only date, leaving hour\n",
    "EventsOcean['date'] = pd.to_datetime(EventsOcean['date']).dt.date\n",
    "\n",
    "# Drop duplicates\n",
    "EventsOcean = EventsOcean.drop_duplicates()\n",
    "\n",
    "# Check dataframe\n",
    "print(f'EventsOcean {EventsOcean.shape} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Nearest weather observation to each ocean incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate nearest weather observation\n",
    "def near_observation(incident):\n",
    "    # Select data corresponding to this Activity_id\n",
    "    coord_incident = EventsOcean[EventsOcean['activity_id'] == incident].iloc[0]\n",
    "\n",
    "    # Select all weather observations for this day\n",
    "    coord_station = marine_stations_daily_2[(marine_stations_daily_2['date'] == coord_incident['date'])]\n",
    "\n",
    "    # Approximate distances\n",
    "    coord_station['station_dist'] = np.sqrt((coord_station['latitude'] - coord_incident['latitude'])**2 +\n",
    "                                            (coord_station['longitude'] - coord_incident['longitude'])**2)\n",
    "\n",
    "    # Return the recorded weather observation located at minimum distance\n",
    "    min_distance_row = coord_station[coord_station['station_dist'] == coord_station['station_dist'].min()]\n",
    "    # Add activity_id to weather data\n",
    "    min_distance_row['activity_id'] = incident\n",
    "\n",
    "    #if coord_station.empty:\n",
    "        #return pd.Series(dtype='float64')\n",
    "    return min_distance_row.drop_duplicates(subset=['activity_id'], keep='first')\n",
    "\n",
    "# Concatenate function returns to create a dataframe\n",
    "if protracted_calculation_enabled :\n",
    "    WeatherOcean = pd.concat([near_observation(incident) for incident in EventsOcean['activity_id']])\n",
    "    print(f'WeatherOcean {WeatherOcean.shape} created')\n",
    "else:\n",
    "    WeatherOcean = pd.read_feather(export_data_folder + '/' + 'WeatherOcean.feather')\n",
    "    print(f'WeatherOcean {WeatherOcean.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to external file\n",
    "if file_export_enabled :\n",
    "    WeatherOcean.reset_index().to_feather(export_data_folder + '/' + 'WeatherOcean.feather')\n",
    "    print(f'WeatherOcean {WeatherOcean.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    WeatherOcean = pd.read_feather(export_data_folder + '/' + 'WeatherOcean.feather')\n",
    "    print(f'WeatherOcean {WeatherOcean.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values printing first observations\n",
    "WeatherOcean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure object\n",
    "fig = go.Figure()\n",
    "\n",
    "# Aggregate WeatherOcean points\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=WeatherOcean['latitude'],\n",
    "    lon=WeatherOcean['longitude'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=np.log1p(WeatherOcean['station_dist']),   # logarithmic scale\n",
    "        colorscale=px.colors.sequential.Viridis,\n",
    "        opacity=0.5,\n",
    "    ),\n",
    "    text=WeatherOcean.apply(lambda row:f\"station:{row['station']}<br>station_dist: {row['station_dist']}\", axis=1),\n",
    "))\n",
    "\n",
    "# Set up map design\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'style': \"open-street-map\",\n",
    "        'center': {'lon': -112, 'lat': 48},\n",
    "        'zoom': 2})\n",
    "\n",
    "# Show map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #2fa4e7;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
