{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis of naval incidents in the USA, 2002 - 2015: <br>\n",
    "## Annex 3.4. Preprocess MergedActivity & VesselBalancedSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: [Oscar Anton](https://www.linkedin.com/in/oscanton/) <br>\n",
    "> Date: 2024 <br>\n",
    "> License: [CC BY-NC-ND 4.0 DEED](https://creativecommons.org/licenses/by-nc-nd/4.0/) <br>\n",
    "> Version: 0.9 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T20:28:05.060427Z",
     "start_time": "2024-05-20T20:28:05.057995Z"
    }
   },
   "outputs": [],
   "source": [
    "# General data management\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T20:28:11.191564Z",
     "start_time": "2024-05-20T20:28:11.181383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main data folders\n",
    "casualty_pollution_folder = 'DataCasualtyAndPollution'\n",
    "weather_ocean_folder = 'DataWeatherOcean'\n",
    "weather_river_folder = 'DataWeatherRiver'\n",
    "merged_activity_folder = 'DataMergedActivity'\n",
    "\n",
    "# Toggle for export data to external file\n",
    "file_export_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load Base Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Events = pd.read_feather(casualty_pollution_folder + '/' + 'Events.feather')\n",
    "print(f'Events {Events.shape} imported from {casualty_pollution_folder}')\n",
    "Vessel = pd.read_feather(casualty_pollution_folder + '/' + 'Vessel.feather')\n",
    "print(f'Vessel {Vessel.shape} imported from {casualty_pollution_folder}')\n",
    "\n",
    "WeatherOcean = pd.read_feather(weather_ocean_folder + '/' + 'WeatherOcean.feather')\n",
    "print(f'WeatherOcean {WeatherOcean.shape} imported from {weather_ocean_folder}')\n",
    "WeatherRiver = pd.read_feather(weather_river_folder + '/' + 'WeatherRiver.feather')\n",
    "print(f'WeatherRiver {WeatherRiver.shape} imported from {weather_ocean_folder}')\n",
    "\n",
    "Injury = pd.read_feather(casualty_pollution_folder + '/' + 'Injury.feather')\n",
    "print(f'Injury {Injury.shape} imported from {casualty_pollution_folder}')\n",
    "VslPoll = pd.read_feather(casualty_pollution_folder + '/' + 'VslPoll.feather')\n",
    "print(f'VslPoll {VslPoll.shape} imported from {casualty_pollution_folder}')\n",
    "\n",
    "Activity = pd.read_feather(casualty_pollution_folder + '/' + 'Activity.feather')\n",
    "print(f'Activity {Activity.shape} imported from {casualty_pollution_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Variable Preselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Events dataframe\n",
    "Events = Events[['activity_id', 'vessel_id', 'vessel_name', 'vessel_class', 'waterway_name', 'event_type', 'damage_status', 'latitude', 'longitude', 'date', 'hour', 'region', 'watertype']]\n",
    "print(f'Events new shape: {Events.shape}')\n",
    "\n",
    "# From the Vessel dataframe. Only include vessels registered in Events\n",
    "Vessel = Vessel[['vessel_id', 'gross_ton', 'length', 'flag_abbr', 'classification_society', 'solas_desc', 'imo_number', 'build_year']]\n",
    "Vessel = Vessel[Vessel['vessel_id'].isin(Events['vessel_id'])]\n",
    "print(f'Vessel new shape: {Vessel.shape}')\n",
    "\n",
    "# From the WeatherOcean dataframe\n",
    "WeatherOcean = WeatherOcean[['activity_id', 'wind_speed', 'visibility', 'air_temp', 'wave_hgt']]\n",
    "print(f'WeatherOcean new shape: {WeatherOcean.shape}')\n",
    "\n",
    "# From the WeatherRiver dataframe\n",
    "WeatherRiver = WeatherRiver.assign(wind_speed=WeatherRiver['awnd'], air_temp=(WeatherRiver['tmax'] + WeatherRiver['tmin']) / 2)\n",
    "WeatherRiver = WeatherRiver[['activity_id', 'wind_speed', 'air_temp']]\n",
    "print(f'WeatherRiver new shape: {WeatherRiver.shape}')\n",
    "\n",
    "# From the Injury dataframe\n",
    "Injury = Injury[['activity_id', 'vessel_id', 'accident_type', 'casualty_type_desc']]\n",
    "print(f'Injury new shape: {Injury.shape}')\n",
    "\n",
    "# From the VslPoll dataframe\n",
    "VslPoll = VslPoll[['activity_id', 'vessel_id', 'chris_cd', 'discharge_amnt_total', 'damage_status']]\n",
    "print(f'VslPoll new shape: {VslPoll.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataframe build: merged_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events and vessel data\n",
    "events_and_vessels = pd.merge(Events, Vessel, how='left', on='vessel_id').drop_duplicates(subset=['activity_id', 'vessel_id', 'event_type'], keep='first')\n",
    "\n",
    "# Variable adaptation\n",
    "events_and_vessels['build_year'] = pd.to_numeric(events_and_vessels['build_year'], errors='coerce')\n",
    "events_and_vessels['gross_ton'] = pd.to_numeric(events_and_vessels['gross_ton'], errors='coerce')\n",
    "events_and_vessels['length'] = pd.to_numeric(events_and_vessels['length'], errors='coerce')\n",
    "events_and_vessels['date'] = pd.to_datetime(events_and_vessels['date']).dt.date\n",
    "\n",
    "# Delete fake 'nan' values\n",
    "events_and_vessels = events_and_vessels.replace('nan', '', regex=True)\n",
    "\n",
    "# Land weather\n",
    "events_river = events_and_vessels[events_and_vessels['watertype'] == 'river']\n",
    "events_river_weather = pd.merge(events_river, WeatherRiver, how='inner', on='activity_id').drop_duplicates()\n",
    "\n",
    "events_river_weather['visibility'] = None\n",
    "events_river_weather['wave_hgt'] = None\n",
    "\n",
    "# Maritime weather\n",
    "events_ocean = events_and_vessels[events_and_vessels['watertype'] == 'ocean']\n",
    "events_ocean_weather = pd.merge(events_ocean, WeatherOcean, how='left', on='activity_id').drop_duplicates()\n",
    "\n",
    "# Vertical union of River + Ocean. Records sorted by date and id\n",
    "merged_activity = pd.concat([events_ocean_weather, events_river_weather]).loc[:, [\n",
    "    'activity_id', 'date', 'hour',\n",
    "    'region', 'latitude', 'longitude',\n",
    "    'watertype', 'event_type', 'damage_status',\n",
    "    'vessel_id', 'imo_number', 'vessel_name', 'vessel_class',\n",
    "    'build_year', 'gross_ton', 'length',\n",
    "    'flag_abbr', 'classification_society', 'solas_desc',\n",
    "    'air_temp', 'wind_speed', 'wave_hgt', 'visibility'\n",
    "]].sort_values(by=['date', 'activity_id']).reset_index(drop=True)\n",
    "\n",
    "# Check dataframe shape\n",
    "print(f'merged_activity {merged_activity.shape} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Add new variables from previous tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damage assessment\n",
    "merged_activity['damage_assessment'] = Events.merge(Activity, on='activity_id', how='left')['damage_assessment']\n",
    "\n",
    "# Personal injuries\n",
    "merged_activity['casualty'] = Events.merge(Injury, on='activity_id', how='left')['casualty_type_desc']\n",
    "\n",
    "# Pollution\n",
    "merged_activity['pollution'] = Events.merge(VslPoll, on='activity_id', how='left')['chris_cd']\n",
    "\n",
    "# Age\n",
    "merged_activity['age'] = pd.to_datetime(merged_activity['date']).dt.year - pd.to_datetime(merged_activity['build_year']).dt.year\n",
    "\n",
    "# Check dataframe shape\n",
    "print(f'merged_activity {merged_activity.shape} updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data quality filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter NAs\n",
    "merged_activity = merged_activity.dropna(thresh=merged_activity.shape[1]-5)\n",
    "\n",
    "# Filter unlikely values\n",
    "merged_activity  = merged_activity [\n",
    "    (merged_activity ['gross_ton'] >= 1) & (merged_activity ['gross_ton'] <= 250000) &\n",
    "    (merged_activity ['build_year'] >= 1800) & (merged_activity ['build_year'] <= 2015) &\n",
    "    (merged_activity ['length'] >= 1) & (merged_activity ['length'] <= 1250)\n",
    "].drop_duplicates(subset=['activity_id', 'vessel_id', 'event_type'], keep='first')\n",
    "\n",
    "# Check dataframe shape\n",
    "print(f'merged_activity {merged_activity.shape} updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Classification model target variable: event_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from event_type to event_class\n",
    "def classify_event(event_type):\n",
    "    if event_type in [\"Sinking\", \"Implosion\", \"Capsize\", \"Loss of Stability\", \"Vessel Maneuverability\", \"Set Adrift\", \"Abandonment\"]:\n",
    "        return \"Critical Events\"\n",
    "    elif event_type in [\"Loss of Electrical Power\", \"Fire\", \"Emergency Response\", \"Explosion\", \"Flooding\", \"Personnel Casualties\", \"Falls into Water\"]:\n",
    "        return \"Onboard Emergencies\"\n",
    "    elif event_type in [\"Grounding\", \"Allision\", \"Collision\"]:\n",
    "        return \"Maritime Accidents\"\n",
    "    elif event_type in [\"Material Failure (Vessels)\", \"Material Failure (Non-vessels)\", \"Material Failure (Diving)\", \"Blowout\"]:\n",
    "        return \"Material Issues\"\n",
    "    elif event_type in [\"Damage to the Environment\", \"Damage to Cargo\", \"Fouling\", \"Evasive Maneuvers\", \"UNSPECIFIED\"]:\n",
    "        return \"Third-party Damages\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply function\n",
    "merged_activity['event_class'] = merged_activity['event_type'].apply(classify_event)\n",
    "\n",
    "# Check new variable counts\n",
    "merged_activity['event_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Export merged_activity dataframe to external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:54:50.226527Z",
     "start_time": "2024-05-20T21:54:49.703337Z"
    }
   },
   "outputs": [],
   "source": [
    "# R Data synchronization\n",
    "import pyreadr\n",
    "merged_activity = pd.DataFrame(pyreadr.read_r(merged_activity_folder + '/' + 'MergedActivity.rds') [None])\n",
    "merged_activity['build_year'] = pd.to_numeric(merged_activity['build_year'], errors='coerce')\n",
    "merged_activity['date'] = pd.to_datetime(merged_activity['date'], errors='coerce')\n",
    "\n",
    "# Export to external file\n",
    "if file_export_enabled :\n",
    "    merged_activity.reset_index().to_feather(merged_activity_folder + '/' + 'merged_activity.feather')\n",
    "    print(f'merged_activity {merged_activity.shape} exported to {merged_activity_folder}')\n",
    "else:\n",
    "    merged_activity = pd.read_feather(merged_activity_folder + '/' + 'merged_activity.feather')\n",
    "    print(f'merged_activity {merged_activity.shape} imported from {merged_activity_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataframe build: vessel_balanced_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all vessel data\n",
    "Vessel = pd.read_feather(casualty_pollution_folder + '/' + 'Vessel.feather')\n",
    "print(f'Vessel {Vessel.shape} imported from {casualty_pollution_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Vessels involved in incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable selection from merged_activity\n",
    "VesselActivity = merged_activity[['vessel_id', 'imo_number', 'vessel_name', 'vessel_class', 'build_year',\n",
    "                                 'gross_ton', 'length', 'flag_abbr', 'classification_society', 'solas_desc',\n",
    "                                 'event_type', 'damage_status']].drop_duplicates()\n",
    "\n",
    "# Check dataframe shape\n",
    "print(f'VesselActivity {VesselActivity.shape} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Vessels not involved in incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find vessels not included in merged_activity\n",
    "VesselNoActivity = Vessel[~Vessel['vessel_id'].isin(merged_activity['vessel_id'])]\n",
    "\n",
    "# Variable adaptation\n",
    "VesselNoActivity['build_year'] = pd.to_numeric(VesselNoActivity['build_year'], errors='coerce')\n",
    "VesselNoActivity['gross_ton'] = pd.to_numeric(VesselNoActivity['gross_ton'], errors='coerce')\n",
    "VesselNoActivity['length'] = pd.to_numeric(VesselNoActivity['length'], errors='coerce')\n",
    "\n",
    "# Filter unlikely values\n",
    "VesselNoActivity = VesselNoActivity [\n",
    "    (VesselNoActivity['gross_ton'] >= 1) & (VesselNoActivity['gross_ton'] <= 250000) &\n",
    "    (VesselNoActivity['build_year'] >= 1800) & (VesselNoActivity['build_year'] <= 2015) &\n",
    "    (VesselNoActivity['length'] >= 1) & (VesselNoActivity['length'] <= 1250)\n",
    "].drop_duplicates(keep='first')\n",
    "\n",
    "# Variable selection\n",
    "VesselNoActivity = VesselNoActivity[['vessel_id', 'imo_number', 'vessel_name', 'vessel_class', 'build_year',\n",
    "                                     'gross_ton', 'length',\n",
    "                                     'flag_abbr', 'classification_society', 'solas_desc']].drop_duplicates()\n",
    "VesselNoActivity['event_type'] = 'No event'\n",
    "VesselNoActivity['damage_status'] = 'Undamaged'\n",
    "\n",
    "# Balanced Sample: same length\n",
    "VesselNoActivitySample = VesselNoActivity.sample(n=len(VesselActivity))\n",
    "\n",
    "# Check dataframe shape\n",
    "print(f'VesselNoActivitySample {VesselNoActivitySample.shape} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Involved and Not involved join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join above dataframes\n",
    "VesselBalancedSample = pd.concat([VesselActivity, VesselNoActivitySample], axis=0)\n",
    "\n",
    "# Check dataframe shape\n",
    "VesselBalancedSample['event_type'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Export dataframe to external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T20:29:13.630929Z",
     "start_time": "2024-05-20T20:29:12.919713Z"
    }
   },
   "outputs": [],
   "source": [
    "# R Data synchronization\n",
    "import pyreadr\n",
    "VesselBalancedSample = pd.DataFrame(pyreadr.read_r(merged_activity_folder + '/' + 'VesselBalancedSample.rds') [None])\n",
    "VesselBalancedSample['build_year'] = pd.to_numeric(VesselBalancedSample['build_year'], errors='coerce')\n",
    "\n",
    "# Export joined dataframe to external file\n",
    "if file_export_enabled :\n",
    "    VesselBalancedSample.reset_index().to_feather(merged_activity_folder + '/' + 'VesselBalancedSample.feather')\n",
    "    print(f'VesselBalancedSample {VesselBalancedSample.shape} exported to {merged_activity_folder}')\n",
    "else:\n",
    "    VesselBalancedSample = pd.read_feather(merged_activity_folder + '/' + 'VesselBalancedSample.feather')\n",
    "    print(f'VesselBalancedSample {VesselBalancedSample.shape} imported to {merged_activity_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Dataframes structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first observations\n",
    "merged_activity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first observations\n",
    "VesselBalancedSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Map visualization (merged_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure object\n",
    "fig = go.Figure()\n",
    "\n",
    "# Aggregate WeatherRiver points\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=merged_activity['latitude'],\n",
    "    lon=merged_activity['longitude'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5,\n",
    "                color=merged_activity['event_class'].map({'Critical Events': 'red',\n",
    "                                                 'Onboard Emergencies': 'orangered',\n",
    "                                                 'Maritime Accidents': 'blue',\n",
    "                                                 'Material Issues': 'yellow',\n",
    "                                                 'Third-party Damages': 'white'}),\n",
    "                opacity=0.5),\n",
    "    text=merged_activity.apply(lambda row:f\"event_class:{row['event_class']}<br>event_type: {row['event_type']}\", axis=1),\n",
    "))\n",
    "\n",
    "# Set up map design\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'style': \"open-street-map\",\n",
    "        'center': {'lon': -112, 'lat': 48},\n",
    "        'zoom': 2})\n",
    "\n",
    "# Show map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #2fa4e7;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
