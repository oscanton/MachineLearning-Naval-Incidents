{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis of naval incidents in the USA, 2002 - 2015: <br>\n",
    "## Annex 3.3. Preprocess Weather River"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: [Oscar Anton](https://www.linkedin.com/in/oscanton/) <br>\n",
    "> Date: 2024 <br>\n",
    "> License: [CC BY-NC-ND 4.0 DEED](https://creativecommons.org/licenses/by-nc-nd/4.0/) <br>\n",
    "> Version: 0.9 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File management\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data folders\n",
    "import_data_folder = 'RawDataWeatherRiver'\n",
    "export_data_folder = 'DataWeatherRiver'\n",
    "\n",
    "# Toggle for export data to external file\n",
    "file_export_enabled = False\n",
    "# Toggle for calculations that takes a long time\n",
    "protracted_calculation_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Decompress and data concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if protracted_calculation_enabled :\n",
    "    # Get the list of files in the folder\n",
    "    files = [file for file in os.listdir(import_data_folder) if file.endswith('.csv.gz')]\n",
    "    \n",
    "    # Initialize an empty DataFrame to be filled with the data from the files\n",
    "    land_stations_comb_1 = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over the files and process each one\n",
    "    for file in files:\n",
    "        file_path = os.path.join(import_data_folder, file)\n",
    "    \n",
    "        # Read the compressed CSV file\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            df_temp = pd.read_csv(file)\n",
    "    \n",
    "            # Select and rename the specific columns\n",
    "            df_temp = df_temp.iloc[:, :4]  # Select the first 4 columns\n",
    "            df_temp.columns = ['STATION', 'DATE', 'ELEMENT', 'DATAVALUE']\n",
    "    \n",
    "            # Filter the DataFrame to include only desired elements\n",
    "            df_temp = df_temp[df_temp['ELEMENT'].isin(['PRCP', 'TMAX', 'TMIN', 'AWND'])]\n",
    "    \n",
    "            # Convert the 'DATE' column to a datetime format\n",
    "            df_temp['DATE'] = pd.to_datetime(df_temp['DATE'], format='%Y%m%d')\n",
    "    \n",
    "            # Pivot the DataFrame to convert it from long to wide format\n",
    "            df_temp = df_temp.pivot(index=['STATION', 'DATE'], columns='ELEMENT', values='DATAVALUE').reset_index()\n",
    "    \n",
    "            # Concatenate with the final DataFrame\n",
    "            land_stations_comb_1 = pd.concat([land_stations_comb_1, df_temp], ignore_index=True)\n",
    "            \n",
    "    # Column names to lowercase\n",
    "    land_stations_comb_1.columns = land_stations_comb_1.columns.str.lower()\n",
    "    print(f'land_stations_comb_1 {land_stations_comb_1.shape} created')\n",
    "else:\n",
    "    land_stations_comb_1 = pd.read_feather(export_data_folder + '/' + 'land_stations_comb_1.feather')\n",
    "    print(f'land_stations_comb_1 {land_stations_comb_1.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or export to external file\n",
    "if file_export_enabled :\n",
    "    land_stations_comb_1.to_feather(export_data_folder + '/' + 'land_stations_comb_1.feather')\n",
    "    print(f'land_stations_comb_1 {land_stations_comb_1.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    land_stations_comb_1 = pd.read_feather(export_data_folder + '/' + 'land_stations_comb_1.feather')\n",
    "    print(f'land_stations_comb_1 {land_stations_comb_1.shape} imported to {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load Station coords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from txt file\n",
    "ghcnd_stations = pd.read_fwf(import_data_folder + '/' + 'ghcnd_stations.txt',\n",
    "                             widths=[11, 9, 10],\n",
    "                             header=None,\n",
    "                             names=[\"STATION\", \"LATITUDE\", \"LONGITUDE\"])\n",
    "\n",
    "# Column names to lowercase\n",
    "ghcnd_stations.columns = ghcnd_stations.columns.str.lower()\n",
    "\n",
    "# Data check\n",
    "print(f'ghcnd_stations {ghcnd_stations.shape} loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Coords to Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Coords\n",
    "land_stations_comb_2 = land_stations_comb_1.merge(ghcnd_stations, how='right', left_on='station', right_on='station')\n",
    "\n",
    "# Only observation with relevant data: No NA in weather variables\n",
    "land_stations_comb_2 = land_stations_comb_2.dropna(subset=['tmax', 'tmin', 'prcp'], thresh=1)\n",
    "\n",
    "# Only Mississippi area\n",
    "land_stations_comb_2 = land_stations_comb_2[(land_stations_comb_2['longitude'] >= -100) &\n",
    "                                            (land_stations_comb_2['longitude'] <= -81.5) &\n",
    "                                            (land_stations_comb_2['latitude'] >= 31) &\n",
    "                                            (land_stations_comb_2['latitude'] <= 49)]\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    land_stations_comb_2.reset_index().to_feather(export_data_folder + '/' + 'land_stations_comb_2.feather')\n",
    "    print(f'land_stations_comb_2 {land_stations_comb_2.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    land_stations_comb_2 = pd.read_feather(export_data_folder + '/' + 'land_stations_comb_2.feather')\n",
    "    print(f'land_stations_comb_2 {land_stations_comb_2.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening: 33% min NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the sum of null values in each row\n",
    "land_stations_comb_3 = land_stations_comb_2.loc[\n",
    "    land_stations_comb_2.isnull().sum(axis=1).sort_values().index]\n",
    "\n",
    "# Select the first rows up to 33% of the total rows\n",
    "percentage_rows = round(0.33 * len(land_stations_comb_3))\n",
    "land_stations_comb_3 = land_stations_comb_3.iloc[:percentage_rows]\n",
    "\n",
    "# Save to external file\n",
    "if file_export_enabled :\n",
    "    land_stations_comb_3.reset_index().to_feather(export_data_folder + '/' + 'land_stations_comb_3.feather')\n",
    "    print(f'land_stations_comb_3 {land_stations_comb_3.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    land_stations_comb_3 = pd.read_feather(export_data_folder + '/' + 'land_stations_comb_3.feather')\n",
    "    print(f'land_stations_comb_3 {land_stations_comb_3.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Weather river data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "land_stations_comb_3 = pd.read_feather(export_data_folder + '/' + 'land_stations_comb_3.feather')\n",
    "\n",
    "# Extract only date, leaving hour\n",
    "land_stations_comb_3['date'] = pd.to_datetime(land_stations_comb_3['date']).dt.date\n",
    "\n",
    "# Variable check\n",
    "land_stations_comb_3['date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Join activity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Load Incidents in Rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "Events = pd.read_feather('DataCasualtyAndPollution' + '/' + 'Events.feather')\n",
    "\n",
    "# Variable selection\n",
    "EventsRiver = Events[(Events.watertype == 'river')][['activity_id', 'date', 'longitude', 'latitude']]\n",
    "\n",
    "# Extract only date, leaving hour\n",
    "EventsRiver['date'] = pd.to_datetime(EventsRiver['date']).dt.date\n",
    "\n",
    "# Drop duplicates\n",
    "EventsRiver = EventsRiver.drop_duplicates()\n",
    "\n",
    "# Data shape check\n",
    "print(f'EventsRiver {EventsRiver.shape} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Nearest weather observation to each river incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate nearest weather observation\n",
    "def near_observation(incident):\n",
    "    # Select data corresponding to this Activity_id\n",
    "    coord_incident = EventsRiver[EventsRiver['activity_id'] == incident].iloc[0]\n",
    "\n",
    "    # Select all weather observations for this day\n",
    "    coord_station = land_stations_comb_3[(land_stations_comb_3['date'] == coord_incident['date'])]\n",
    "\n",
    "    # Approximate distances\n",
    "    coord_station['station_dist'] = np.sqrt((coord_station['latitude'] - coord_incident['latitude'])**2 +\n",
    "                                            (coord_station['longitude'] - coord_incident['longitude'])**2)\n",
    "\n",
    "    # Return the recorded weather observation located at minimum distance\n",
    "    min_distance_row = coord_station[coord_station['station_dist'] == coord_station['station_dist'].min()]\n",
    "    # Add activity_id to weather data\n",
    "    min_distance_row['activity_id'] = incident\n",
    "\n",
    "    #if coord_station.empty:\n",
    "        #return pd.Series(dtype='float64')\n",
    "    return min_distance_row.drop_duplicates(subset=['activity_id'], keep='first')\n",
    "\n",
    "# Concatenate function returns to create a dataframe\n",
    "if protracted_calculation_enabled :\n",
    "    WeatherRiver = pd.concat([near_observation(incident) for incident in EventsRiver['activity_id']])\n",
    "    print(f'WeatherRiver {WeatherRiver.shape} created')\n",
    "else:\n",
    "    WeatherRiver = pd.read_feather(export_data_folder + '/' + 'WeatherRiver.feather')\n",
    "    print(f'WeatherRiver {WeatherRiver.shape} imported from {export_data_folder}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to external file\n",
    "if file_export_enabled :\n",
    "    WeatherRiver.reset_index().to_feather(export_data_folder + '/' + 'WeatherRiver.feather')\n",
    "    print(f'WeatherRiver {WeatherRiver.shape} exported to {export_data_folder}')\n",
    "else:\n",
    "    WeatherRiver = pd.read_feather(export_data_folder + '/' + 'WeatherRiver.feather')\n",
    "    print(f'WeatherRiver {WeatherRiver.shape} imported from {export_data_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data check: Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first observations\n",
    "WeatherRiver.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure object\n",
    "fig = go.Figure()\n",
    "\n",
    "# Aggregate WeatherRiver points\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=WeatherRiver['latitude'],\n",
    "    lon=WeatherRiver['longitude'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=np.log1p(WeatherRiver['station_dist']),   # logarithmic scale\n",
    "        colorscale=px.colors.sequential.Viridis,\n",
    "        opacity=0.5,\n",
    "    ),\n",
    "    text=WeatherRiver.apply(lambda row:f\"station:{row['station']}<br>station_dist: {row['station_dist']}\", axis=1),\n",
    "))\n",
    "\n",
    "# Set up map design\n",
    "fig.update_layout(\n",
    "    margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "    mapbox = {\n",
    "        'style': \"open-street-map\",\n",
    "        'center': {'lon': -112, 'lat': 48},\n",
    "        'zoom': 2})\n",
    "\n",
    "# Show map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #2fa4e7;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
