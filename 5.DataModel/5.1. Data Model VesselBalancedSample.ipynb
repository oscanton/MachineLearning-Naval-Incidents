{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis of naval incidents in the USA, 2002 - 2015: <br>\n",
    "## Annex 5.1. Data Model VesselBalancedSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: [Oscar Anton](https://www.linkedin.com/in/oscanton/) <br>\n",
    "> Date: 2024 <br>\n",
    "> License: [CC BY-NC-ND 4.0 DEED](https://creativecommons.org/licenses/by-nc-nd/4.0/) <br>\n",
    "> Version: 0.9 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System environment\n",
    "import os\n",
    "\n",
    "# Data general management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "# Model training\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, ShuffleSplit, learning_curve\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, f1_score, mean_absolute_error, roc_auc_score, roc_curve, auc, cohen_kappa_score, confusion_matrix\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Model Export\n",
    "import joblib\n",
    "\n",
    "# Model explainers\n",
    "import dalex as dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Main data folders\n",
    "merged_activity_folder = '../3.DataPreprocess/DataMergedActivity'\n",
    "datasets_folder = 'Datasets'\n",
    "models_folder = 'Models'\n",
    "\n",
    "# Toggle for export data to external file\n",
    "file_export_enabled = False\n",
    "# Toggle for train model\n",
    "train_model_enabled = False\n",
    "\n",
    "# Available CPU cores for multiprocessing (training models)\n",
    "n_jobs = os.cpu_count() - 1\n",
    "# Random seed for reproducibility\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "VesselBalancedSample = pd.read_feather(merged_activity_folder + '/' + 'VesselBalancedSample.feather')\n",
    "\n",
    "# Check dataframe\n",
    "VesselBalancedSample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataframe creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Variable transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Missing and NA values previously processed. Data balanced by design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Category variables: Reduce excessive variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: cutting values\n",
    "def cut_years(column):\n",
    "    return pd.cut(column,\n",
    "                  bins=[-float('inf'), 1940, 1960, 1980, 2000, float('inf')],\n",
    "                  labels=['very Old', 'old', 'average', 'new', 'very new'],\n",
    "                  include_lowest=True)\n",
    "\n",
    "# Function: group minority values \n",
    "def lump_factorials(column, prop=0.008, other_level=\"other value\"):\n",
    "    counts = column.value_counts(normalize=True)\n",
    "    mask = column.isin(counts[counts < prop].index)\n",
    "    column[mask] = other_level\n",
    "    return column\n",
    "\n",
    "# First transformations: assign 'y': No event = 0, otherwise = 1\n",
    "# Cutting, renaming, dropping not relevant variables\n",
    "data_general = (\n",
    "    VesselBalancedSample\n",
    "    .assign(y=lambda x: pd.factorize(x['event_type'] != \"No event\", sort=True)[0])\n",
    "    .assign(build_year=cut_years(VesselBalancedSample['build_year']))\n",
    "    .rename(columns={'length': 'vessel_length'})\n",
    "    .drop(columns=['vessel_id', 'imo_number', 'vessel_name', 'event_type', 'damage_status'])\n",
    ")\n",
    "\n",
    "# Group minority values \n",
    "data_general[['vessel_class', 'flag_abbr', 'classification_society', 'solas_desc']] = (data_general[['vessel_class', 'flag_abbr', 'classification_society', 'solas_desc']]\n",
    "    .apply(lump_factorials)\n",
    ")\n",
    "\n",
    "# Check structure\n",
    "data_general.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Category variables: one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for not numeric variables\n",
    "data_ohe = (pd.get_dummies(data_general\n",
    "                .select_dtypes(exclude=['number']))\n",
    "                .astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Numeric variables: Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standard scaler\n",
    "data_scaled = pd.DataFrame(StandardScaler()\n",
    "               .fit_transform(data_general[['gross_ton', 'vessel_length']])\n",
    ")\n",
    "\n",
    "# Rename column names as strings\n",
    "data_scaled.columns = ['gross_ton', 'vessel_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset join: encoded, scaled and 'y'\n",
    "data_num = pd.concat([data_ohe,\n",
    "                     data_scaled,\n",
    "                     data_general['y']],\n",
    "                     axis=1)\n",
    "\n",
    "# Rename column names as strings\n",
    "data_num.columns = data_num.columns.astype(str)\n",
    "\n",
    "# Verify variables\n",
    "for column in data_num.columns:\n",
    "    print(f\"Name: {column} | Type: {data_num[column].dtype} | Levels: {data_num[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "if file_export_enabled :\n",
    "    data_num.reset_index().to_feather(datasets_folder + '/' + 'data_num.feather')\n",
    "    print(f'data_num {data_num.shape} exported to {datasets_folder}')\n",
    "else:\n",
    "    data_num = pd.read_feather(datasets_folder + '/' + 'data_num.feather')\n",
    "    print(f'data_num {data_num.shape} imported from {datasets_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    data_num.drop(columns=['y']),\n",
    "                                    data_num['y'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / Load dataframes in one file (h5 format for multiple data)\n",
    "if file_export_enabled :\n",
    "    dfs = {'X_train':X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test}\n",
    "    for key, df in dfs.items():\n",
    "        df.to_hdf(datasets_folder + '/' + 'datasets_splited.h5', key=key, format='table')\n",
    "        print(f'{key} {eval(key).shape} exported to {datasets_folder} folder') \n",
    "else:\n",
    "    dfs = ['X_train', 'X_test', 'y_train', 'y_test']\n",
    "    for df in dfs:\n",
    "        globals()[df] = pd.read_hdf(datasets_folder + '/' + 'datasets_splited.h5', key = df)\n",
    "        print(f'{df} {eval(df).shape} imported from {datasets_folder} folder') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance displaying functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with main metrics data\n",
    "def model_metrics(model, X, y):\n",
    "    # Predictions (absolute)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate main metrics\n",
    "    roc_auc = round(roc_auc_score(y, y_pred), 4)\n",
    "    accuracy = round(accuracy_score(y, y_pred), 4)\n",
    "    kappa = round(cohen_kappa_score(y, y_pred), 4)\n",
    "    rmse = round(mean_squared_error(y, y_pred), 4)\n",
    "    mae = round(mean_absolute_error(y, y_pred), 4)\n",
    "    r2 = round(r2_score(y, y_pred), 4)\n",
    "    f1 = round(f1_score(y, y_pred), 4)\n",
    "\n",
    "    # Sensitivity And Specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    sensitivity =  round(tp / (tp + fn), 4)\n",
    "    specificity = round(tn / (tn + fp), 4)\n",
    "    \n",
    "    # Build multiindex table\n",
    "    metrics_df = pd.DataFrame([['ROC AUC:', roc_auc],['Accuracy:', accuracy], ['Kappa:', kappa],\n",
    "                            ['RMSE:', rmse], ['MAE:', mae], ['R2:', r2], ['F1:', f1],[' ', ' '],\n",
    "                            ['Sensitivity:', sensitivity], ['Specificity:', specificity]],\n",
    "                            columns=pd.MultiIndex.from_product([[model.__class__.__name__],['Metric', 'Value']]))\n",
    "    \n",
    "    return metrics_df.style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with Confusion Matrix data\n",
    "def confusion_matrix_table(model, X, y):\n",
    "    # Predictions (absolute)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "    # Dataframe creation\n",
    "    df = pd.DataFrame([[tp, fn],[fp, tn]],\n",
    "                  index=pd.Index(['1', '0'], name='Actual Label:'),\n",
    "                  columns=pd.MultiIndex.from_product([[model.__class__.__name__],['1', '0']], names=['Model:', 'Predicted:']))\n",
    "\n",
    "    # Dataframe style\n",
    "    styled_df = df.style.set_table_styles([\n",
    "        {'selector': 'th.col_heading', 'props': 'text-align: center;'},\n",
    "        {'selector': 'td', 'props': 'text-align: center;'},\n",
    "    ], overwrite=False)\n",
    "\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Plot ROC Curve\n",
    "def plot_roc_curve(model, X, y):\n",
    "    # Predicted probabilities\n",
    "    y_score = model.predict_proba(X)\n",
    "    \n",
    "    # Calculate ROC for each class\n",
    "    fpr, tpr, _ = roc_curve(y, y_score[:, 1])\n",
    "    \n",
    "    # Calculate AUC (Area Under Curve)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='dotted')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {model.__class__.__name__}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bayesian networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Naïve Bayes -Gaussian- (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {}                         # No parameters for this model\n",
    "\n",
    "    # Create and train model\n",
    "    model = GaussianNB(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'nb_train.pkl')\n",
    "    \n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'nb_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Bagged Naïve Bayes -Gaussian- (BNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {}                         # No parameters for this model\n",
    "\n",
    "    # Create and train model. \n",
    "    model = BaggingClassifier(GaussianNB(**params), n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'bnb_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'bnb_train.pkl')\n",
    "\n",
    "# Bagging (Bootstrap Aggregating) is a technique used to improve the stability and accuracy.\n",
    "# n_estimators specifies how many individual Naive Bayes classifiers are trained independently on subsets of the training data.\n",
    "# It approximates the behavior of AODE\n",
    "    \n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Gradient Boosting Machine (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'n_estimators': 500,            # Number of trees\n",
    "        'learning_rate': 0.1,           # Contribution of each tree to the model\n",
    "        'max_depth': 3,                 # Maximum levels of each tree\n",
    "        'random_state': seed,           # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = GradientBoostingClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'GBM_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'GBM_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Main metrics per iteration (n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations during training\n",
    "iterations = int(model.get_params().get(\"n_estimators\") + 1)\n",
    "\n",
    "# Obtain prediction probabilities at each stage of training\n",
    "train_probs = np.array([preds[:, 1] for preds in model.staged_predict_proba(X_train)])\n",
    "test_probs = np.array([preds[:, 1] for preds in model.staged_predict_proba(X_test)])\n",
    "\n",
    "# Calculate ROC metric at each stage of training\n",
    "train_roc_auc = [roc_auc_score(y_train, prob) for prob in train_probs]\n",
    "test_roc_auc = [roc_auc_score(y_train, prob) for prob in train_probs]\n",
    "\n",
    "# Plot the ROC metric as a function of iterations\n",
    "plt.plot(range(1, iterations), train_roc_auc, label=\"Training ROC AUC\", color=\"blue\")\n",
    "plt.plot(range(1, iterations), test_roc_auc, label=\"Test ROC AUC\", color=\"red\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.title(\"ROC AUC Metric per iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions at each stage of training\n",
    "train_errors = [accuracy_score(y_train, y_pred) for y_pred in model.staged_predict(X_train)]\n",
    "test_errors = [accuracy_score(y_test, y_pred) for y_pred in model.staged_predict(X_test)]\n",
    "\n",
    "# Plot the Accuracy metric as a function of iterations\n",
    "plt.plot(np.arange(1, iterations), train_errors, label=\"Training Accuracy\", color=\"blue\")\n",
    "plt.plot(np.arange(1, iterations), test_errors, label=\"Test Accuracy\", color=\"red\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Metric per iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions at each stage of training\n",
    "train_kappa = [cohen_kappa_score(y_train, y_pred) for y_pred in model.staged_predict(X_train)]\n",
    "test_kappa = [cohen_kappa_score(y_test, y_pred) for y_pred in model.staged_predict(X_test)]\n",
    "\n",
    "# Plot the Kappa metric as a function of iterations\n",
    "plt.plot(np.arange(1, iterations), train_kappa, label=\"Training Kappa\", color=\"blue\")\n",
    "plt.plot(np.arange(1, iterations), test_kappa, label=\"Test Kappa\", color=\"red\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Kappa\")\n",
    "plt.title(\"Kappa Metric per iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities = []\n",
    "specificities = []\n",
    "\n",
    "# Get values per each iteration \n",
    "for i, y_pred in enumerate(model.staged_predict(X_test)):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Plot sensitivity and specificity for each iteration of training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, iterations), sensitivities, label='Sensitivity', color='blue')\n",
    "plt.plot(range(1, iterations), specificities, label='Specificity', color='green')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sensitivity and Specificity per Training Iteration')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Extreme Gradient Boosting (XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "    'objective': 'binary:logistic',     # Binary classification problem\n",
    "    'eval_metric': 'error',             # Evaluation metric: error rate\n",
    "    'eta': 0.1,                         # Learning rate\n",
    "    'max_depth': 3,                     # Maximum tree depth\n",
    "    'colsample_bytree': 0.3,            # Subsample ratio of columns when constructing each tree\n",
    "    'random_state': seed                # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = XGBClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'XGB_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'XGB_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. More models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Random Forest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :   \n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'n_estimators': 100,            # Number of trees in the forest\n",
    "        'max_depth': None,              # Maximum depth of the trees (no restrictions)\n",
    "        'random_state': seed            # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = RandomForestClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'rf_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'rf_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Main metrics per iteration (n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of iterations, initial number of estimators, and step size\n",
    "n_iterations = 10\n",
    "initial_n_estimators = 10\n",
    "step_size = 10\n",
    "\n",
    "# Create empty lists to store the number of estimators and ROC AUC scores\n",
    "num_estimators_list = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Iterate over the specified number of iterations\n",
    "for i in range(n_iterations):\n",
    "    # Create the Random Forest model with the current number of estimators\n",
    "    model = RandomForestClassifier(n_estimators=initial_n_estimators + i * step_size, random_state=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and calculate ROC AUC score\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store the number of estimators and ROC AUC score\n",
    "    num_estimators_list.append(initial_n_estimators + i * step_size)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "# Plot the ROC AUC scores over the number of estimators\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_estimators_list, roc_auc_scores, label=\"ROC AUC\", color=\"red\")\n",
    "plt.title('ROC AUC Score vs. Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the number of estimators and ROC AUC scores\n",
    "num_estimators_list = []\n",
    "accuracy_auc_scores = []\n",
    "\n",
    "# Iterate over the specified number of iterations\n",
    "for i in range(n_iterations):\n",
    "    # Create the Random Forest model with the current number of estimators\n",
    "    model = RandomForestClassifier(n_estimators=initial_n_estimators + i * step_size, random_state=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate Accuracy scores\n",
    "    accuracy_auc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the number of estimators and Accuracy score\n",
    "    num_estimators_list.append(initial_n_estimators + i * step_size)\n",
    "    accuracy_auc_scores.append(accuracy_auc)\n",
    "\n",
    "# Plot the Accuracy scores over the number of estimators\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_estimators_list, accuracy_auc_scores, label=\"Accuracy\", color=\"red\")\n",
    "plt.title('Accuracy Score vs. Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the number of estimators and ROC AUC scores\n",
    "num_estimators_list = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Iterate over the specified number of iterations\n",
    "for i in range(n_iterations):\n",
    "    # Create the Random Forest model with the current number of estimators\n",
    "    model = RandomForestClassifier(n_estimators=initial_n_estimators + i * step_size, random_state=seed)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate Kappa scores\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the number of estimators and Kappa scores\n",
    "    num_estimators_list.append(initial_n_estimators + i * step_size)\n",
    "    kappa_scores.append(kappa)\n",
    "\n",
    "# Plot the Kappa scores over the number of estimators\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_estimators_list, accuracy_auc_scores, label=\"Kappa\", color=\"red\")\n",
    "plt.title('Kappa Score vs. Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Kappa Score')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Explore structure of a tree example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_export_enabled :\n",
    "    # Extract a random model tree (for example, the first one)\n",
    "    tree = model.estimators_[0]\n",
    "\n",
    "    # Export the tree to Graphviz\n",
    "    dot_data = export_graphviz(tree, out_file=None, \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True, precision=5)  \n",
    "    graph = graphviz.Source(dot_data)\n",
    "\n",
    "    # Export to pdf file (lighter than svg)\n",
    "    graph.render('rf_train_tree', format='pdf', cleanup=True)\n",
    "\n",
    "# Note: File is not displayed because is extremely width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Support Vector Machine -Radial Kernel- (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'kernel': 'rbf',                # Radial Kernel\n",
    "        'probability': True,            # Enable probability estimates (uses 5-fold cross-validation)\n",
    "        'random_state': seed            # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = SVC(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'svm_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'svm_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'hidden_layer_sizes': (100, 50),    # Two hidden layers with 100 and 50 neurons respectively.\n",
    "        'activation': 'relu',               # Activation function for the hidden layers: Rectified Linear Unit\n",
    "        'solver': 'adam',                   # Optimization algorithm\n",
    "        'random_state': seed                # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = MLPClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'nnet_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'nnet_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Classification and Regression Trees (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'criterion':'gini',                 # Criteria\n",
    "        'random_state': seed                # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = DecisionTreeClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'cart_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'cart_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'random_state': seed                # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    model = LogisticRegression(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(model, models_folder + '/' + 'rl_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    model = joblib.load(models_folder + '/' + 'rl_train.pkl')\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Main Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show main metrics\n",
    "model_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to show confusion matrix\n",
    "confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to plot Receiver Operating Characteristic (ROC) curve\n",
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model list\n",
    "models = ['nb_train', 'bnb_train', 'GBM_train',\n",
    "          'XGB_train', 'rf_train', 'svm_train',\n",
    "          'nnet_train', 'cart_train', 'rl_train']\n",
    "\n",
    "# Load all models\n",
    "for model in models:\n",
    "    globals()[model] = joblib.load(models_folder + '/' + model + '.pkl')\n",
    "    print(f'{model} loaded from {models_folder} folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected models list\n",
    "selected_models = ['GBM_train', 'XGB_train', 'rf_train', 'cart_train']\n",
    "\n",
    "for model in selected_models:\n",
    "# Plot feature importances\n",
    "    importances = pd.DataFrame({'value':eval(model).feature_importances_,\n",
    "                                'variable_name':eval(model).feature_names_in_}).sort_values(by='value', ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    plt.title(f\"Feature importances of {eval(model)}\")\n",
    "    plt.barh(importances['variable_name'], importances['value'], color='#00bfc4', align='center')\n",
    "    plt.xlabel('Relative Feature importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Performance of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of main metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions for all models\n",
    "y_preds = [eval(model).predict(X_test) for model in models]\n",
    "\n",
    "# Sensitivity & Specificity\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_preds[i]).ravel()\n",
    "    sensitivity.append(round(tp / (tp + fn), 4))\n",
    "    specificity.append(round(tn / (tn + fp), 4))\n",
    "\n",
    "# Dataframe for all metrics, sorted by ROC AUC\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Model': [model.split('_')[0].upper() for model in models],\n",
    "    'ROC AUC': [round(roc_auc_score(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'Accuracy': [round(accuracy_score(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'Kappa': [round(cohen_kappa_score(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'RMSE': [round(mean_squared_error(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'MAE': [round(mean_absolute_error(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'R2': [round(r2_score(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'F1': [round(f1_score(y_test, y_preds[i]), 4) for i in range(len(models))],\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity\n",
    "}).sort_values(['ROC AUC'], ascending=[False]).style.hide()\n",
    "\n",
    "# Show table\n",
    "metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for Cross Validation (10 splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_export_enabled :\n",
    "    # Calculate scores for accuracy for all models\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    accuracy_scores = []\n",
    "    for model in models:\n",
    "        accuracy_scores.append(cross_val_score(eval(model), X_train, y_train, scoring='accuracy', cv=kfold, n_jobs=n_jobs))\n",
    "\n",
    "    # Store scores in a pandas dataframe and export to external file\n",
    "    accuracy_scores = pd.DataFrame(accuracy_scores)\n",
    "    accuracy_scores.columns = accuracy_scores.columns.astype(str)\n",
    "    accuracy_scores.reset_index().to_feather(datasets_folder + '/' + 'accuracy_scores.feather')\n",
    "\n",
    "else:\n",
    "    # Load this dataframe otherwise\n",
    "    accuracy_scores = pd.read_feather(datasets_folder + '/' + 'accuracy_scores.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots of accuracy values in cross validation\n",
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "fig.suptitle('Cross Validation Accuracy')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(accuracy_scores[::-1].T.iloc[1:].reset_index(drop=True), vert=False)\n",
    "ax.set_yticklabels([model.split('_')[0].upper() for model in models][::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert score data to list\n",
    "score_list = accuracy_scores.values.tolist()\n",
    "\n",
    "# Initiate table\n",
    "table=[]\n",
    "for i in range(len(score_list)):\n",
    "    table.append([])\n",
    "for i in range(len(score_list)):\n",
    "    for j in range(len(score_list)):\n",
    "        table[i].append(0)\n",
    "\n",
    "# Populate values   \n",
    "for i in range(len(score_list)): \n",
    "    for j in range(i+1,len(score_list)):\n",
    "        stat, p = ttest_rel(score_list[i], score_list[j])\n",
    "      \n",
    "        alpha = 0.05\n",
    "        \n",
    "        table[i][j]=np.round(p,3) # upper diagonal\n",
    "        table[j][i]=np.round(stat,3) # lower diagonal\n",
    "        table[i][i]=''\n",
    "\n",
    "print('The following table shows the comparison of the statistics and the p-values of the models')\n",
    "print('The value of the statistic on the lower diagonal and the p-value on the upper diagonal')\n",
    "\n",
    "# Show table, including model names\n",
    "table = pd.DataFrame(table).set_index(pd.Index([model.split('_')[0].upper() for model in models]), inplace=False)\n",
    "table.columns = [model.split('_')[0].upper() for model in models]\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curve(model, X, y, cv, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Learning Curve of {model}\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=cv, train_sizes=train_sizes, n_jobs=n_jobs)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "# Set sample split\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=seed)\n",
    "\n",
    "for model in selected_models:\n",
    "# Plot learning curves\n",
    "    plot_learning_curve(eval(model), X_train, y_train, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dalex explainers dict for selected models\n",
    "explainers = {}\n",
    "for model in selected_models:\n",
    "    explainers[model] = dx.Explainer(eval(model), X_train, y_train, label=model.split('_')[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Residual Diagnostics\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_diagnostics().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Residual Diagnostics (Absolute)\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_diagnostics().plot(variable = \"ids\", yvariable = \"abs_residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Global Explainability - Variable importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variable importances for selected models\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_parts().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Partial Dependence Profile (PDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDP for selected models\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_profile(variables = [\"vessel_class_Recreational\", \"vessel_class_Towing Vessel\"] , type = \"partial\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Break Down profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample observation\n",
    "vessel_sample = X_train.sample(n=1, random_state=seed)\n",
    "\n",
    "# Show this observation\n",
    "vessel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shapley values for selected observation\n",
    "for explainer in explainers.values():\n",
    "    explainer.predict_parts(new_observation = vessel_sample, type = \"break_down\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. SHapley Additive exPlanations (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions (time-consuming process)\n",
    "if train_model_enabled :\n",
    "    shaps = {}\n",
    "    for model in explainers:\n",
    "        shaps[model] = explainers[model].predict_parts(new_observation = vessel_sample, type = \"shap\")\n",
    "    joblib.dump(shaps, models_folder + '/' + 'shaps.pkl')\n",
    "else:\n",
    "    shaps = joblib.load(models_folder + '/' + 'shaps.pkl')\n",
    "\n",
    "# Plot shapley values\n",
    "for model in explainers:\n",
    "     shaps[model].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Ceteris Paribus profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample observation\n",
    "vessel_sample = X_train.sample()\n",
    "\n",
    "# Plot shapley values for selected observation\n",
    "for explainer in explainers.values():\n",
    "    explainer.predict_profile(new_observation = vessel_sample).plot(variables = [\"vessel_class_Recreational\", \"vessel_class_Towing Vessel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version\n",
    "import sys\n",
    "print(f'Python version: {sys.version}')\n",
    "\n",
    "# Initiate variables for package versions\n",
    "pkgs_dict = {}\n",
    "version = []\n",
    "\n",
    "# Loop importing __version__ for each package (because no whole packages are imported)\n",
    "for pkg in ['pandas', 'numpy', 'sklearn', 'dalex']:\n",
    "    exec(f\"from {pkg} import __version__ as version\")\n",
    "    pkgs_dict[pkg] = version\n",
    "    \n",
    "# Show as table\n",
    "pd.DataFrame(pkgs_dict, index=[\"Package version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #2fa4e7;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
