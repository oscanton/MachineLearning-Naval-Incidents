{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis of naval incidents in the USA, 2002 - 2015: <br>\n",
    "## Annex 5.2. Data Model MergedActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: [Oscar Anton](https://www.linkedin.com/in/oscanton/) <br>\n",
    "> Date: 2024 <br>\n",
    "> License: [CC BY-NC-ND 4.0 DEED](https://creativecommons.org/licenses/by-nc-nd/4.0/) <br>\n",
    "> Version: 0.9 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System environment\n",
    "import os\n",
    "\n",
    "# Data general management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample balance\n",
    "import cube                                     # https://github.com/acdmammoths/parallelcubesampling\n",
    "from imblearn.over_sampling import SMOTENC      # https://github.com/scikit-learn-contrib/imbalanced-learn\n",
    "\n",
    "# NaN imputation\n",
    "from sklearn.experimental import enable_iterative_imputer       # Needed for IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# Data scaling\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "# Data splitting\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "# Model training\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# h2o framework\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "# Model metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, f1_score, mean_absolute_error, roc_auc_score, roc_curve, auc, cohen_kappa_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "# Model Export\n",
    "import joblib\n",
    "\n",
    "# Model explainers\n",
    "import dalex as dx\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Main data folders\n",
    "merged_activity_folder = '../3.DataPreprocess/DataMergedActivity'\n",
    "datasets_folder = 'Datasets'\n",
    "models_folder = 'Models'\n",
    "\n",
    "# Toggle for export data to external file\n",
    "file_export_enabled = False\n",
    "# Toggle for train model\n",
    "train_model_enabled = False\n",
    "\n",
    "# Available CPU cores for multiprocessing (training models)\n",
    "n_jobs = os.cpu_count() - 1\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "merged_activity = pd.read_feather(merged_activity_folder + '/' + 'merged_activity.feather')\n",
    "\n",
    "# Check dataframe\n",
    "merged_activity.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataframe creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Variable transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary variables for the models\n",
    "columns_to_drop = ['vessel_id', 'imo_number', 'vessel_name', 'event_type', \n",
    "                   'build_year', 'wave_hgt', 'visibility', 'casualty', \n",
    "                   'pollution', 'flag_abbr', 'classification_society', 'solas_desc']\n",
    "\n",
    "merged_activity = merged_activity.drop(columns=columns_to_drop)\n",
    "\n",
    "# Renaming columns\n",
    "merged_activity = merged_activity.rename(columns={'length': 'vessel_length', 'event_class': 'y'})\n",
    "\n",
    "# Date & Time variables to continuous data\n",
    "merged_activity['date'] = merged_activity['date'].dt.dayofyear\n",
    "\n",
    "merged_activity['hour'] = (pd.to_numeric(merged_activity['hour'].str.split(':').str[0]) + \n",
    "                         pd.to_numeric(merged_activity['hour'].str.split(':').str[1])/60).round(2)\n",
    "\n",
    "# Function: group minority values \n",
    "def lump_factorials(column, prop=0.008, other_level=\"other value\"):\n",
    "    counts = column.value_counts(normalize=True)\n",
    "    mask = column.isin(counts[counts < prop].index)\n",
    "    column[mask] = other_level\n",
    "    return column\n",
    "\n",
    "# Apply function for reducing variability in vessel_class\n",
    "merged_activity[['vessel_class']] = merged_activity[['vessel_class']].apply(lump_factorials)\n",
    "\n",
    "# Converting some columns to factors (categorical)\n",
    "columns_to_factorize = ['region', 'watertype', 'damage_status', 'vessel_class', 'y']\n",
    "merged_activity[columns_to_factorize] = merged_activity[columns_to_factorize].astype('category')\n",
    "\n",
    "# Check structure\n",
    "merged_activity.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable frecuency\n",
    "merged_activity['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Target variable balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Subsampling: Cube Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_subsampling(data, target_size):\n",
    "    # Drop all incomplete cases if not lower than target size (keeping best information)\n",
    "    if len(data.dropna()) >= target_size:\n",
    "        data = data.dropna()\n",
    "\n",
    "    # Constant column of \"1s\"\n",
    "    ONE = np.full(len(data), 1)\n",
    "\n",
    "    # Select numerical variables\n",
    "    X1 = data.select_dtypes(include=['number']).reset_index(drop=True)\n",
    "\n",
    "    # Categorical variables to numerical (one hot encoding)\n",
    "    X2 = pd.get_dummies(data.drop(columns='y').select_dtypes(exclude=['number']).reset_index(drop=True))\n",
    "\n",
    "    # Join data\n",
    "    X = np.column_stack((ONE, X1, X2))\n",
    "\n",
    "    # Inclusion probability (constant according to target size)\n",
    "    init_probs = np.full(len(X), target_size / len(X))\n",
    "\n",
    "    # Inclusion index (algorithm by Alexander Lee, Stefan Walzer-Goldfeld, Shukry Zablah, Matteo Riondato, AAAI'22 Student Abstract)\n",
    "    sample_indexes = cube.sample_cube_parallel(X, init_probs, n_jobs, is_pop_size_fixed=True, is_sample_size_fixed=True, seed=seed)\n",
    "\n",
    "    # Return selected rows according sample_indexes\n",
    "    return data.loc[sample_indexes == 1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critical events: Subsampling to 9000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call cube_subsampling for y = Critical Events \n",
    "CriticalEvents_cube = cube_subsampling(merged_activity[merged_activity['y'] == 'Critical Events'], 9000)\n",
    "\n",
    "# Check sampled length\n",
    "print(f\"Critical Events subsample dimensions: {CriticalEvents_cube.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maritime Accidents: Subsampling to 9000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call cube_subsampling for y = Critical Events \n",
    "MaritimeAccidents_cube = cube_subsampling(merged_activity[merged_activity['y'] == 'Maritime Accidents'], 9000)\n",
    "\n",
    "# Check sampled length\n",
    "print(f\"Maritime Accidents subsample dimensions: {MaritimeAccidents_cube.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Material Issues: Subsampling to 9000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call cube_subsampling for y = Critical Events \n",
    "MaterialIssues_cube = cube_subsampling(merged_activity[merged_activity['y'] == 'Material Issues'], 9000)\n",
    "\n",
    "# Check sampled length\n",
    "print(f\"Material Issues subsample dimensions: {MaterialIssues_cube.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. NaN imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this case NaN imputation is before oversampling because used algorithm needs complete cases to work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with NaN\n",
    "nan_columns = ['air_temp', 'wind_speed', 'damage_assessment'] \n",
    "\n",
    "# MICE (Multiple Imputation by Chained Equations ) imputation\n",
    "imputer = IterativeImputer(random_state=seed)\n",
    "imputed_values = imputer.fit_transform(merged_activity[nan_columns])\n",
    "\n",
    "# Imputation apply\n",
    "imputed_data = merged_activity.copy()\n",
    "imputed_data[nan_columns] = imputed_values\n",
    "\n",
    "# Check imputation\n",
    "imputed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Oversampling: SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split target variable\n",
    "y = imputed_data['y']\n",
    "X = imputed_data.drop(columns=['y'])\n",
    "\n",
    "# Set target size for variables to be oversampled\n",
    "target_size = 9000\n",
    "\n",
    "# Create SMOTE-NC (Synthetic Minority Over-sampling Technique for Nominal and Continuous)\n",
    "smote = SMOTENC(sampling_strategy={'Onboard Emergencies': target_size, 'Third-party Damages': target_size},\n",
    "categorical_features=[4, 7, 8, 9],\n",
    "random_state=seed)\n",
    "\n",
    "# Apply SMOTE-NC fit\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Back to join data\n",
    "resampled_data = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "# Check balance\n",
    "resampled_data['y'].value_counts()\n",
    "\n",
    "# Extract resampled 'Onboard Emergencies' class\n",
    "onboard_emergencies_smote = resampled_data[resampled_data['y'] == 'Onboard Emergencies']\n",
    "# Check sampled length\n",
    "print(f\"Onboard Emergencies oversample dimensions: {onboard_emergencies_smote.shape}\")\n",
    "\n",
    "# Extract resampled 'Third-party Damages' class\n",
    "thirdparty_damages_smote = resampled_data[resampled_data['y'] == 'Third-party Damages']\n",
    "# Check sampled length\n",
    "print(f\"Third-party Damages oversample dimensions: {thirdparty_damages_smote.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. All events data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled pieces join\n",
    "merged_activity_general = pd.concat([CriticalEvents_cube, MaritimeAccidents_cube, MaterialIssues_cube,\n",
    "                                         onboard_emergencies_smote, thirdparty_damages_smote]).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop statistically irrelevant variables, according to EDA\n",
    "merged_activity_general = merged_activity_general.drop(columns=['date', 'latitude', 'damage_assessment'])\n",
    "\n",
    "# Export to external file\n",
    "if file_export_enabled :\n",
    "    (merged_activity_general\n",
    "    .reset_index(drop=True)\n",
    "    .to_feather(datasets_folder + '/' + 'merged_activity_general.feather'))\n",
    "else:\n",
    "    merged_activity_general = pd.read_feather(datasets_folder + '/' + 'merged_activity_general.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Explanatory variables: Scale numerics & OHE not numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric variables\n",
    "data_scaled = pd.DataFrame(StandardScaler()\n",
    "                            .fit_transform(merged_activity_general\n",
    "                            .select_dtypes(include=['number'])))\n",
    "\n",
    "# Rename numeric column names\n",
    "data_scaled.columns = merged_activity_general.select_dtypes(include=['number']).columns\n",
    "\n",
    "\n",
    "# One hot encoding for not numeric variables\n",
    "data_ohe = (pd.get_dummies(merged_activity_general\n",
    "                .drop(columns=['y'])\n",
    "                .select_dtypes(exclude=['number']))\n",
    "                .astype(int))\n",
    "\n",
    "\n",
    "# X: Scaled and ohe join\n",
    "X = pd.concat([data_ohe, data_scaled], axis=1).drop(columns=['index'])\n",
    "\n",
    "# Verify variables\n",
    "for column in X.columns:\n",
    "    print(f\"Name: {column} | Type: {X[column].dtype} | Levels: {X[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Target variable: Numeric Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = pd.Series(label_encoder.fit_transform(merged_activity_general['y']))\n",
    "\n",
    "# Verify\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save / Load dataframes in one file (h5 format for multiple data)\n",
    "if file_export_enabled :\n",
    "    dfs = {'X_train':X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test}\n",
    "    for key, df in dfs.items():\n",
    "        df.to_hdf(datasets_folder + '/' + 'datasets_MA_splited.h5', key=key, format='table')\n",
    "        print(f'{key} {df.shape} saved') \n",
    "else:\n",
    "    dfs = ['X_train', 'X_test', 'y_train', 'y_test']\n",
    "    for df in dfs:\n",
    "        globals()[df] = pd.read_hdf(datasets_folder + '/' + 'datasets_MA_splited.h5', key = df)\n",
    "        print(f'{df} {eval(df).shape} loaded') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance functions (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with main metrics data\n",
    "def ma_model_metrics(model, X, y, styled=False):\n",
    "    # Predictions (absolute)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Data binarize for auc calculation\n",
    "    y_bin = label_binarize(y , classes=np.unique(y))\n",
    "    y_pred_bin = label_binarize(y_pred , classes=np.unique(y))\n",
    "\n",
    "    # Calculate main metrics\n",
    "    roc_auc = round(roc_auc_score(y_bin, y_pred_bin), 4)\n",
    "    accuracy = round(accuracy_score(y, y_pred), 4)\n",
    "    kappa = round(cohen_kappa_score(y, y_pred), 4)\n",
    "    rmse = round(mean_squared_error(y, y_pred), 4)\n",
    "    mae = round(mean_absolute_error(y, y_pred), 4)\n",
    "    r2 = round(r2_score(y, y_pred), 4)\n",
    "    f1 = round(f1_score(y, y_pred, average='macro'), 4)\n",
    "    \n",
    "    # Build multiindex table\n",
    "    df = pd.DataFrame([['ROC AUC:', roc_auc],['Accuracy:', accuracy], ['Kappa:', kappa],\n",
    "                        ['RMSE:', rmse], ['MAE:', mae], ['R2:', r2], ['F1:', f1]],\n",
    "                        columns=('metric', 'value'))\n",
    "\n",
    "    if styled:\n",
    "        title = f'{model.__class__.__name__} Training'           \n",
    "        df.columns = pd.MultiIndex.from_tuples([(title, col) for col in df.columns])\n",
    "        return df.style.hide()\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table for recall & precision, sensitivity & specificity\n",
    "def sens_spec(model, X, y, styled=False):\n",
    "    # Predictions (absolute)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Recall & Precision values\n",
    "    recall = round(recall_score(y, y_pred, average='macro'), 4)\n",
    "    precision = round(precision_score(y, y_pred, average='macro'), 4)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "    # List compression for sens & spec values calculation\n",
    "    sensitivity, specificity = zip(*[(round(recall_score(y, y_pred, labels=[i], average='macro'), 4),\n",
    "                                    round(conf_matrix[i, i] / sum(conf_matrix[:, i]), 4))\n",
    "                                    for i in range(len(conf_matrix))])\n",
    "\n",
    "    # Labels and indexes\n",
    "    column_labels = label_encoder.inverse_transform(model.classes_)\n",
    "    index_1 = ['Recall:', 'Precision:']\n",
    "    index_2 = [recall, precision]\n",
    "    index_ = [' - ',' - ']\n",
    "    index_3 = ['Sensitivity:', 'Specificity:']\n",
    "\n",
    "    # Build multiindex table\n",
    "    df = pd.DataFrame([sensitivity, specificity])\n",
    "    df.columns = column_labels\n",
    "    df.index = [index_1, index_2, index_, index_3]\n",
    "\n",
    "    # Dataframe style\n",
    "    if styled:\n",
    "        title = f'{model.__class__.__name__} Model'           \n",
    "        df.columns = pd.MultiIndex.from_tuples([(title, col) for col in df.columns])\n",
    "        return df.style.set_table_styles([{'selector': 'th.col_heading', 'props': 'text-align: center;'}], overwrite=False)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with Confusion Matrix\n",
    "def ma_confusion_matrix_table(model, X, y):\n",
    "    # Predictions (absolute)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Get labels decoding target variable \n",
    "    labels = label_encoder.inverse_transform(model.classes_)\n",
    "\n",
    "    # Build table\n",
    "    df = pd.DataFrame(confusion_matrix(y, y_pred),\n",
    "                            columns=pd.MultiIndex.from_product([[f'{model.__class__.__name__}: Confusion Matrix'], labels]),\n",
    "                            index=labels)\n",
    "\n",
    "    # Dataframe style\n",
    "    styled_df = df.style.set_table_styles([\n",
    "        {'selector': 'th.col_heading', 'props': 'text-align: center;'},\n",
    "        {'selector': 'td', 'props': 'text-align: center;'},\n",
    "    ], overwrite=False)\n",
    "\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Plot Multiclass ROC Curve\n",
    "def roc_curve_plot(model, X, y):\n",
    "    # Predictions (absolute)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Data binarize for auc calculation\n",
    "    y_bin = label_binarize(y , classes=model.classes_)\n",
    "    y_pred_bin = label_binarize(y_pred , classes=model.classes_)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in model.classes_:\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_pred_bin.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure()\n",
    "    colors = sns.color_palette(\"hls\", 5)  \n",
    "    for i, color in zip(model.classes_, colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label='ROC of class {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='grey', lw=1, linestyle='dashed',\n",
    "            label='micro-average ROC (AUC = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='dotted')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC (Multiclass) for {model.__class__.__name__}')\n",
    "    plt.legend(loc=\"lower right\", fontsize=\"8\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Feature importances\n",
    "def sklearn_feature_importances(model, plot=False):\n",
    "    importances = pd.DataFrame({'variable_name':model.feature_names_in_,\n",
    "                                    'value':model.feature_importances_}).sort_values(by='value', ascending=False)\n",
    "    # Plot horizontal bars if enabled in the call, otherwise return values\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.barh(importances['variable_name'], importances['value'], color='#00bfc4')\n",
    "        plt.title(f\"Feature importances of {model.__class__.__name__} model\")\n",
    "        plt.xlabel('Relative Feature importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    else:\n",
    "        return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bayesian networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Na√Øve Bayes -Gaussian- (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "\n",
    "    # Define model parameters\n",
    "    params = {}                         # No parameters for this model\n",
    "\n",
    "    # Create and train model\n",
    "    nb_MA_train = GaussianNB(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(nb_MA_train, models_folder + '/' + 'nb_MA_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    nb_MA_train = joblib.load(models_folder + '/' + 'nb_MA_train.pkl')\n",
    "\n",
    "model = nb_MA_train\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "ma_model_metrics(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "sens_spec(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "ma_confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Receiver Operating Characteristic (ROC) curves for each class\n",
    "roc_curve_plot(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_theta_feature_importances(model, plot=False):\n",
    "    X = model.feature_names_in_\n",
    "    num_classes = model.classes_.size\n",
    "    \n",
    "    # Iterate through the classes and sum the absolute values of the corresponding theta parameters\n",
    "    importances = []\n",
    "    for i in range(len(X)):\n",
    "        importance = 0\n",
    "        for j in range(num_classes):\n",
    "            importance += np.abs(model.theta_[j, i])\n",
    "        importances.append((X[i], importance))\n",
    "    \n",
    "    # Normalize importance values\n",
    "    total_importance = sum(x[1] for x in importances)\n",
    "    importances = [(x[0], x[1] / total_importance) for x in importances]\n",
    "    \n",
    "    # Built importance values dataframe\n",
    "    importances = pd.DataFrame(importances, columns=['variable_name', 'value']).sort_values(by='value', ascending=False)\n",
    "\n",
    "    # Plot horizontal bars if enabled in the call, otherwise return values\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.barh(importances['variable_name'], importances['value'], color='#00bfc4')\n",
    "        plt.title(f\"Feature importances of {model.__class__.__name__} model\")\n",
    "        plt.xlabel('Relative Feature importance (based on theta values)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    else:\n",
    "        return importances\n",
    "\n",
    "\n",
    "# Plot features importances calling above function\n",
    "sklearn_theta_feature_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Gradient Boosting Machine (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'n_estimators': 500,            # Number of trees\n",
    "        'learning_rate': 0.1,           # Contribution of each tree to the model\n",
    "        'max_depth': 3,                 # Maximum levels of each tree\n",
    "        'random_state': seed,           # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    GBM_MA_train = GradientBoostingClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(GBM_MA_train, models_folder + '/' + 'GBM_MA_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    GBM_MA_train = joblib.load(models_folder + '/' + 'GBM_MA_train.pkl')\n",
    "\n",
    "model = GBM_MA_train\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "ma_model_metrics(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "sens_spec(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "ma_confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Receiver Operating Characteristic (ROC) curves for each class\n",
    "roc_curve_plot(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_feature_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. More Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Random Forest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :   \n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'n_estimators': 100,            # Number of trees in the forest\n",
    "        'max_depth': None,              # Maximum depth of the trees (no restrictions)\n",
    "        'random_state': seed            # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    rf_MA_train = RandomForestClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(rf_MA_train, models_folder + '/' + 'rf_MA_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    rf_MA_train = joblib.load(models_folder + '/' + 'rf_MA_train.pkl')\n",
    "\n",
    "model = rf_MA_train\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "ma_model_metrics(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "sens_spec(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "ma_confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Receiver Operating Characteristic (ROC) curves for each class\n",
    "roc_curve_plot(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_feature_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'hidden_layer_sizes': (100, 50),    # Two hidden layers with 100 and 50 neurons respectively.\n",
    "        'activation': 'relu',               # Activation function for the hidden layers: Rectified Linear Unit\n",
    "        'solver': 'adam',                   # Optimization algorithm\n",
    "        'random_state': seed                # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    nnet_MA_train = MLPClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(nnet_MA_train, models_folder + '/' + 'nnet_MA_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    nnet_MA_train = joblib.load(models_folder + '/' + 'nnet_MA_train.pkl')\n",
    "\n",
    "model = nnet_MA_train\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "ma_model_metrics(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "sens_spec(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "ma_confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Receiver Operating Characteristic (ROC) curves for each class\n",
    "roc_curve_plot(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_coef_feature_importances(model, plot=False):\n",
    "    # Built coefs dataframe\n",
    "    importances = pd.DataFrame({'variable_name':model.feature_names_in_,\n",
    "                                'value':np.mean(np.abs(model.coefs_[0].T), axis=0)}).sort_values(by='value', ascending=False)\n",
    "    # Normalize values\n",
    "    importances['value'] = importances['value'] / importances['value'].sum()\n",
    "\n",
    "    # Plot horizontal bars if enabled in the call, otherwise return values\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.barh(importances['variable_name'], importances['value'], color='#00bfc4')\n",
    "        plt.title(f\"Feature importances of {model.__class__.__name__} model\")\n",
    "        plt.xlabel('Relative Feature importance (based on  first layer coef values)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    else:\n",
    "        return importances\n",
    "    \n",
    "# Call above function\n",
    "sklearn_coef_feature_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Classification and Regression Trees (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'criterion':'gini',                 # Criteria\n",
    "        'random_state': seed                # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Create and train model\n",
    "    cart_MA_train = DecisionTreeClassifier(**params).fit(X_train, y_train)\n",
    "\n",
    "    # Save to external file\n",
    "    joblib.dump(cart_MA_train, models_folder + '/' + 'cart_MA_train.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    cart_MA_train = joblib.load(models_folder + '/' + 'cart_MA_train.pkl')\n",
    "\n",
    "model = cart_MA_train\n",
    "\n",
    "# Get model parameters and print as a one row list\n",
    "print(f'Model name: {model.__class__.__name__} \\nParameters:')\n",
    "params = model.get_params()\n",
    "for param_name, param_value in params.items():\n",
    "    print(f\" {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "ma_model_metrics(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "sens_spec(model, X_test, y_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "ma_confusion_matrix_table(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Receiver Operating Characteristic (ROC) curves for each class\n",
    "roc_curve_plot(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_feature_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Keras API with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance functions (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Plot loss / accuracy train evolution\n",
    "def keras_train_plot(data):\n",
    "    # Train process visualization\n",
    "    df_train=pd.DataFrame(data)\n",
    "    # df_train['epochs']=history.epoch\n",
    "    df_train['epochs']=list(range(0, len(data['accuracy'])))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "    fig.suptitle('Train process', fontsize=12)\n",
    "\n",
    "    ax1.plot(df_train['epochs'], df_train['accuracy'], label='train_accuracy')\n",
    "    ax1.plot(df_train['epochs'], df_train['val_accuracy'], label='val_accuracy')\n",
    "\n",
    "    ax2.plot(df_train['epochs'], df_train['loss'], label='train_loss')\n",
    "    ax2.plot(df_train['epochs'], df_train['val_loss'], label='val_loss')\n",
    "\n",
    "    ax1.legend(loc='best')\n",
    "    ax2.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with main metrics data\n",
    "def keras_model_metrics(model, X, y_ohe, styled=False):\n",
    "    y_pred_ohe = pd.DataFrame(model.predict(X))              \n",
    "    y_pred_serie = y_pred_ohe.idxmax(axis=1) \n",
    "    y_serie = pd.Series(label_encoder.fit_transform(y_ohe.idxmax(axis=1)))\n",
    "\n",
    "    roc_auc = round(roc_auc_score(y_ohe, y_pred_ohe), 4)\n",
    "    accuracy = round(accuracy_score(y_serie, y_pred_serie), 4)\n",
    "    kappa = round(cohen_kappa_score(y_serie, y_pred_serie), 4)\n",
    "    rmse = round(mean_squared_error(y_ohe, y_pred_ohe), 4)\n",
    "    mae = round(mean_absolute_error(y_ohe, y_pred_ohe), 4)\n",
    "    r2 = round(r2_score(y_ohe, y_pred_ohe), 4)\n",
    "    f1 = round(f1_score(y_serie, y_pred_serie, average='macro'), 4)\n",
    "\n",
    "    # Build multiindex table\n",
    "    df = pd.DataFrame([['ROC AUC:', roc_auc],['Accuracy:', accuracy], ['Kappa:', kappa],\n",
    "                        ['RMSE:', rmse], ['MAE:', mae], ['R2:', r2], ['F1:', f1]],\n",
    "                        columns=('metric', 'value'))\n",
    "\n",
    "    if styled:\n",
    "        title = f'{model.__class__.__name__} Training'           \n",
    "        df.columns = pd.MultiIndex.from_tuples([(title, col) for col in df.columns])\n",
    "        return df.style.hide()\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table for recall & precision, sensitivity & specificity\n",
    "def keras_sens_spec(model, X, y, styled=False):\n",
    "    # Predictions (absolute)\n",
    "    y_pred_ohe = pd.DataFrame(model.predict(X))              \n",
    "    y_pred_serie = y_pred_ohe.idxmax(axis=1) \n",
    "    y_serie = pd.Series(label_encoder.fit_transform(y.idxmax(axis=1)))\n",
    "\n",
    "    # Recall & Precision values\n",
    "    recall = round(recall_score(y_serie, y_pred_serie, average='macro'), 4)\n",
    "    precision = round(precision_score(y_serie, y_pred_serie, average='macro'), 4)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_serie, y_pred_serie)\n",
    "\n",
    "    # List compression for sens & spec values calculation\n",
    "    sensitivity, specificity = zip(*[(round(recall_score(y_serie, y_pred_serie, labels=[i], average='macro'), 4),\n",
    "                                    round(conf_matrix[i, i] / sum(conf_matrix[:, i]), 4))\n",
    "                                    for i in range(len(conf_matrix))])\n",
    "\n",
    "    # Labels and indexes\n",
    "    column_labels = y.columns\n",
    "    index_1 = ['Recall:', 'Precision:']\n",
    "    index_2 = [recall, precision]\n",
    "    index_ = [' - ',' - ']\n",
    "    index_3 = ['Sensitivity:', 'Specificity:']\n",
    "\n",
    "    # Build multiindex table\n",
    "    df = pd.DataFrame([sensitivity, specificity])\n",
    "    df.columns = column_labels\n",
    "    df.index = [index_1, index_2, index_, index_3]\n",
    "\n",
    "    # Dataframe style\n",
    "    if styled:\n",
    "        title = f'{model.__class__.__name__} Model'           \n",
    "        df.columns = pd.MultiIndex.from_tuples([(title, col) for col in df.columns])\n",
    "        return df.style.set_table_styles([{'selector': 'th.col_heading', 'props': 'text-align: center;'}], overwrite=False)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with Confusion Matrix\n",
    "def keras_confusion_matrix_table(model, X, y):\n",
    "    # Predictions (max)\n",
    "    y_pred_max = np.argmax(model.predict(X), axis=1)\n",
    "    y_max = np.argmax(y, axis=1)\n",
    "\n",
    "    # Build table\n",
    "    df = pd.DataFrame(confusion_matrix(y_max, y_pred_max),\n",
    "                            columns=pd.MultiIndex.from_product([[f'{model.name}: Confusion Matrix'], y.columns]),\n",
    "                            index=y.columns)\n",
    "\n",
    "    # Dataframe style\n",
    "    styled_df = df.style.set_table_styles([\n",
    "        {'selector': 'th.col_heading', 'props': 'text-align: center;'},\n",
    "        {'selector': 'td', 'props': 'text-align: center;'},\n",
    "    ], overwrite=False)\n",
    "\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Plot Multiclass ROC Curve\n",
    "def keras_roc_curve_plot(model, X, y):\n",
    "    # Predictions (max)\n",
    "    y_pred = pd.DataFrame(model.predict(X))\n",
    "    y_pred.columns = label_encoder.inverse_transform(y_pred.columns)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in y.columns:\n",
    "        fpr[i], tpr[i], _ = roc_curve(y[i], y_pred[i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.values.ravel(), y_pred.values.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure()\n",
    "    colors = sns.color_palette(\"hls\", 5)  \n",
    "    for i, color in zip(y.columns, colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label='ROC of class {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='grey', lw=1, linestyle='dashed',\n",
    "            label='micro-average ROC (AUC = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='dotted')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC (Multiclass) for {model.name}')\n",
    "    plt.legend(loc=\"lower right\", fontsize=\"8\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Sequential: Densely-connected Model (Simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoder for target variable\n",
    "#y_ohe = (pd.get_dummies(y).astype(int))\n",
    "y_ohe = pd.get_dummies(y).astype(int)\n",
    "y_ohe.columns = label_encoder.inverse_transform(y_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "keras_model_1 = Sequential()\n",
    "keras_model_1.add(Input(shape=(X.shape[1],)))           # Input layer: shape = Feature data shape\n",
    "keras_model_1.add(Dense(128, activation='relu'))        # Dense layer, relu activation\n",
    "keras_model_1.add(Dropout(0.5))                         # Dropout\n",
    "keras_model_1.add(Dense(64, activation='relu')) \n",
    "keras_model_1.add(Dropout(0.5))\n",
    "keras_model_1.add(Dense(y_ohe.shape[1], activation='softmax'))\n",
    "\n",
    "model = keras_model_1\n",
    "\n",
    "# Check structure\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X, y_ohe,\n",
    "            epochs=50,\n",
    "            batch_size = 32,\n",
    "            callbacks = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True),\n",
    "            validation_split=0.15)\n",
    "    \n",
    "    # Save model and history to external files\n",
    "    model.save(models_folder + '/' + 'keras_model_1.keras')\n",
    "    joblib.dump(history.history, models_folder + '/' + 'keras_model_1_evolution.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model and history from external files\n",
    "    model = load_model(models_folder + '/' + 'keras_model_1.keras')\n",
    "    class EmplyClass:\n",
    "        def __init__(self, history):\n",
    "            self.history = history\n",
    "    history = EmplyClass('')\n",
    "    history.history = joblib.load(models_folder + '/' + 'keras_model_1_evolution.pkl')\n",
    "    print(f'Model <{model.name}> and train history loaded from {models_folder} folder')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy / loss Train evolution\n",
    "keras_train_plot(history.history)\n",
    "\n",
    "# Accuracy / loss values\n",
    "accuracy, loss = model.evaluate(X, y_ohe)\n",
    "print(\"accuracy = {:.4f}, loss = {:.4f}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "keras_model_metrics(model, X, y_ohe, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "keras_sens_spec(model, X, y_ohe, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "keras_confusion_matrix_table(model, X, y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Roc curves\n",
    "keras_roc_curve_plot(model, X, y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Plot feature importances\n",
    "def keras_sec_importances(model, X=X, plot=False):\n",
    "    # Calculate input layer weights\n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "    # Create dataframe for weights and variable names\n",
    "    importances = pd.DataFrame({'variable_name':X.columns, 'value':np.mean(np.abs(weights), axis=1)}).sort_values(by='value', ascending=False)\n",
    "\n",
    "    # Plot horizontal bars if enabled in the call, otherwise return values\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.barh(importances['variable_name'], importances['value'], color='#00bfc4')\n",
    "        plt.title(f\"Feature importances in Keras {model.__class__.__name__} model\")\n",
    "        plt.xlabel('Relative Feature importance (based on first layer weights)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    else:\n",
    "        return importances\n",
    "    \n",
    "\n",
    "# Plot features importances calling above function\n",
    "keras_sec_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Functional API Model (Multi-input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename similar column names (vessel_...)\n",
    "X_renamed = X.rename(columns=lambda x: 'class' if x.startswith('vessel_class') else ('length' if x.startswith('vessel_length') else x))\n",
    "\n",
    "# Group the columns by the first three letters of their name\n",
    "groups = X_renamed.groupby(lambda x: x[:3], axis=1)\n",
    "\n",
    "# Iteration over each group and create a DataFrame for each one\n",
    "X_splited = {}\n",
    "for name, group in groups:\n",
    "    X_splited[name] = group\n",
    "\n",
    "# Verify all splited variable names\n",
    "print(f'X_splited keys: {X_splited.keys()}')\n",
    "\n",
    "# Convert to list for ease of use\n",
    "X_joined = list(X_splited.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functional_model():\n",
    "    # Numerical Input layers: Dense + dropout layers\n",
    "    input_act = Input(shape=(X_splited['act'].shape[1],))\n",
    "    dense_act = Dense(128, activation='relu', name='activity_id')(input_act)\n",
    "    dropout_act = Dropout(0.1)(dense_act)\n",
    "\n",
    "    input_age = Input(shape=(X_splited['age'].shape[1],))\n",
    "    dense_age = Dense(128, activation='relu', name='age')(input_age)\n",
    "    dropout_age = Dropout(0.1)(dense_age)\n",
    "\n",
    "    input_air = Input(shape=(X_splited['air'].shape[1],))\n",
    "    dense_air = Dense(128, activation='relu', name='air_temp')(input_air)\n",
    "    dropout_air = Dropout(0.1)(dense_air)\n",
    "\n",
    "    input_gro = Input(shape=(X_splited['gro'].shape[1],))\n",
    "    dense_gro = Dense(128, activation='relu', name='gross_ton')(input_gro)\n",
    "    dropout_gro = Dropout(0.1)(dense_gro)\n",
    "\n",
    "    input_hou = Input(shape=(X_splited['hou'].shape[1],))\n",
    "    dense_hou = Dense(128, activation='relu', name='hour')(input_hou)\n",
    "    dropout_hou = Dropout(0.1)(dense_hou)\n",
    "\n",
    "    input_len = Input(shape=(X_splited['len'].shape[1],))\n",
    "    dense_len = Dense(128, activation='relu', name='vessel_length')(input_len)\n",
    "    dropout_len = Dropout(0.1)(dense_len)\n",
    "\n",
    "    input_lon = Input(shape=(X_splited['lon'].shape[1],))\n",
    "    dense_lon = Dense(128, activation='relu', name='longitude')(input_lon)\n",
    "    dropout_lon = Dropout(0.1)(dense_lon)\n",
    "\n",
    "    input_win = Input(shape=(X_splited['win'].shape[1],))\n",
    "    dense_win = Dense(128, activation='relu', name='wind_speed')(input_win)\n",
    "    dropout_win = Dropout(0.1)(dense_win)\n",
    "\n",
    "    # Categorical Input layers (one-hot): Dense layers\n",
    "    input_dam = Input(shape=(X_splited['dam'].shape[1],))\n",
    "    dense_dam = Dense(128, activation='linear', name='damage_status')(input_dam)\n",
    "\n",
    "    input_cla = Input(shape=(X_splited['cla'].shape[1],))\n",
    "    dense_cla = Dense(128, activation='linear', name='vessel_class')(input_cla)\n",
    "\n",
    "    input_wat = Input(shape=(X_splited['wat'].shape[1],))\n",
    "    dense_wat = Dense(128, activation='linear', name='water_type')(input_wat)\n",
    "\n",
    "    input_reg = Input(shape=(X_splited['reg'].shape[1],))\n",
    "    dense_reg = Dense(128, activation='linear', name='region')(input_reg)\n",
    "\n",
    "\n",
    "    # First level joins\n",
    "    input_gro_len = concatenate([dropout_gro, dropout_len])\n",
    "    dense_gro_len = Dense(64, activation='relu')(input_gro_len)\n",
    "\n",
    "    input_lon_reg = concatenate([dropout_lon, dense_reg])\n",
    "    dense_lon_reg = Dense(64, activation='relu')(input_lon_reg)\n",
    "\n",
    "    input_air_win = concatenate([dropout_air, dropout_win])\n",
    "    dense_air_win = Dense(64, activation='relu')(input_air_win)\n",
    "\n",
    "    # Second level joins\n",
    "    input_wat_lon_reg = concatenate([dense_lon_reg, dense_wat])\n",
    "    dense_wat_lon_reg = Dense(64, activation='relu')(input_wat_lon_reg)\n",
    "\n",
    "    # Third level joins\n",
    "    input_gro_len_age_cla = concatenate([dense_gro_len, dropout_age, dense_cla])\n",
    "    dense_gro_len_age_cla = Dense(64, activation='relu')(input_gro_len_age_cla)\n",
    "\n",
    "    # Fourth level joins\n",
    "    input_wat_lon_reg_air_win = concatenate([dense_wat_lon_reg, dense_air_win])\n",
    "    dense_wat_lon_reg_air_win = Dense(64, activation='relu')(input_wat_lon_reg_air_win)\n",
    "\n",
    "    # Fifth level joins\n",
    "    input_gather = concatenate([dropout_act, dense_dam, dense_gro_len_age_cla, dropout_hou, dense_wat_lon_reg_air_win])\n",
    "    dense_gather = Dense(64, activation='relu')(input_gather)\n",
    "\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(y_ohe.shape[1], activation='softmax')(dense_gather)\n",
    "\n",
    "\n",
    "    # Model definition\n",
    "    model = Model(inputs=[input_act, input_age, input_air, input_cla,\n",
    "                        input_dam, input_gro, input_hou, input_len,\n",
    "                        input_lon, input_reg, input_wat, input_win], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "keras_model_2 = functional_model()\n",
    "\n",
    "model = keras_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit([X_splited['act'], X_splited['age'], X_splited['air'], X_splited['cla'],\n",
    "                        X_splited['dam'], X_splited['gro'], X_splited['hou'], X_splited['len'],\n",
    "                        X_splited['lon'], X_splited['reg'], X_splited['wat'], X_splited['win']], y_ohe,\n",
    "            epochs=50,\n",
    "            batch_size = 32,\n",
    "            callbacks = EarlyStopping(monitor = 'val_loss', patience = 15, restore_best_weights = True),\n",
    "            validation_split=0.15)\n",
    "\n",
    "    # Save model and history to external files\n",
    "    model.save(models_folder + '/' + 'keras_model_2.keras')\n",
    "    joblib.dump(history.history, models_folder + '/' + 'keras_model_2_evolution.pkl')\n",
    "\n",
    "else:\n",
    "    # Load model and history from external files\n",
    "    model = load_model(models_folder + '/' + 'keras_model_2.keras')\n",
    "    class EmplyClass:\n",
    "        def __init__(self, history):\n",
    "            self.history = history\n",
    "    history = EmplyClass('')\n",
    "    history.history = joblib.load(models_folder + '/' + 'keras_model_2_evolution.pkl')\n",
    "    print(f'Model <{model.name}> and train history loaded from {models_folder} folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy / loss Train evolution\n",
    "keras_train_plot(history.history)\n",
    "\n",
    "# Accuracy / loss values\n",
    "accuracy, loss = model.evaluate(X_joined, y_ohe)\n",
    "\n",
    "print(\"accuracy = {:.4f}, loss = {:.4f}\".format(accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance main metrics\n",
    "keras_model_metrics(model, X_joined, y_ohe, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall & precision, sensitivity & specificity\n",
    "keras_sens_spec(model, X_joined, y_ohe, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "keras_confusion_matrix_table(model, X_joined, y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Roc curves\n",
    "keras_roc_curve_plot(model, X_joined, y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Plot feature importances\n",
    "def keras_func_importances(model, plot = False):\n",
    "    weight_data = []\n",
    "    # Considering all entrance dense layers have a name like corresponding variable name\n",
    "    # Iterate through dense layers with a particular name, obtaining their weights\n",
    "    for layer in model.layers:\n",
    "        if 'Dense' in layer.__class__.__name__ and 'dense_' not in layer.name:\n",
    "            layer_name = layer.name\n",
    "            weights = np.mean(np.abs(layer.get_weights()[0]), axis=1)\n",
    "            for i, weight in enumerate(weights):\n",
    "                weight_data.append([f\"{layer_name}_{i}\", weight])\n",
    "\n",
    "    # Build dataframe\n",
    "    importances = pd.DataFrame(weight_data, columns=['variable_name', 'value']).sort_values(by='value', ascending=False)\n",
    "\n",
    "    # Plot horizontal bars if enabled in the call, otherwise return values\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.barh(importances['variable_name'], importances['value'], color='#00bfc4')\n",
    "        plt.title(f\"Feature importances in Keras {model.__class__.__name__} model\")\n",
    "        plt.xlabel('Relative Feature importance (based on first layer weights)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    else:\n",
    "        return importances\n",
    "\n",
    "\n",
    "# Plot features importances calling above function\n",
    "keras_func_importances(model, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. H2o AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than 1 attempt to init h2o cluster (to avoid problems)\n",
    "def start_h2o_cluster():\n",
    "    try:\n",
    "        h2o.init()  # Attempt to start the H2O cluster\n",
    "        return True  # Return True if the cluster starts successfully\n",
    "    except Exception as e:\n",
    "        return False  # Return False if there is an error starting the cluster\n",
    "\n",
    "attempts = 0\n",
    "max_attempts = 3  # Maximum number of attempts\n",
    "\n",
    "while attempts < max_attempts:\n",
    "    if start_h2o_cluster():\n",
    "        break  # If the cluster starts successfully, exit the loop\n",
    "    else:\n",
    "        attempts += 1\n",
    "\n",
    "if attempts == max_attempts:\n",
    "    print(\"Failed to start H2O cluster after\", max_attempts, \"attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to h2o format\n",
    "h2o_data = h2o.H2OFrame(merged_activity_general)\n",
    "\n",
    "# Train / Test split\n",
    "h2o_train, h2o_test = h2o_data.split_frame(ratios=[0.85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model_enabled :\n",
    "    # AutoMachineLearning Model \n",
    "    aml = H2OAutoML(seed=seed, max_models=4, keep_cross_validation_predictions=True, exclude_algos=['StackedEnsemble'],verbosity=\"info\")     # Alternative: max_runtime_secs = 2000\n",
    "\n",
    "    # Train\n",
    "    h2o_model = aml.train(y='y', training_frame=h2o_train)\n",
    "\n",
    "    # Save to external file\n",
    "    h2o.save_model(model=h2o_model, path=models_folder, filename = 'mod_aml.h2o', force=True)\n",
    "\n",
    "else:\n",
    "    # Load model from external file\n",
    "    h2o_model = h2o.load_model(models_folder + '/' + 'mod_aml.h2o')\n",
    "    print(f'Model {h2o_model.key} loaded from {models_folder} folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model details\n",
    "h2o_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table with main metrics data\n",
    "def h2o_model_metrics(h2o_model, h2o_test, styled=False):\n",
    "    h2o_predict = pd.Series(label_encoder.fit_transform(h2o_model.predict(h2o_test)['predict'].as_data_frame()))\n",
    "    h2o_y = pd.Series(label_encoder.fit_transform(h2o_test['y'].as_data_frame()))\n",
    "\n",
    "    h2o_y_bin = label_binarize(h2o_y , classes=np.unique(h2o_y))\n",
    "    h2o_predict_bin = label_binarize(h2o_predict , classes=np.unique(h2o_y))\n",
    "\n",
    "    # Calculate main metrics\n",
    "    roc_auc = round(roc_auc_score(h2o_y_bin, h2o_predict_bin), 4)\n",
    "    accuracy = round(accuracy_score(h2o_y_bin, h2o_predict_bin), 4)\n",
    "    kappa = round(cohen_kappa_score(h2o_y, h2o_predict), 4)\n",
    "    rmse = round(mean_squared_error(h2o_y_bin, h2o_predict_bin), 4)\n",
    "    mae = round(mean_absolute_error(h2o_y_bin, h2o_predict_bin), 4)\n",
    "    r2 = round(r2_score(h2o_y_bin, h2o_predict_bin), 4)\n",
    "    f1 = round(f1_score(h2o_y_bin, h2o_predict_bin, average='macro'), 4)\n",
    "\n",
    "    # Build multiindex table\n",
    "    df = pd.DataFrame([['ROC AUC:', roc_auc],['Accuracy:', accuracy], ['Kappa:', kappa],\n",
    "                        ['RMSE:', rmse], ['MAE:', mae], ['R2:', r2], ['F1:', f1]],\n",
    "                        columns=('metric', 'value'))\n",
    "\n",
    "    if styled:\n",
    "        title = f'{h2o_model.key} Training'           \n",
    "        df.columns = pd.MultiIndex.from_tuples([(title, col) for col in df.columns])\n",
    "        return df.style.hide()\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Call above function\n",
    "h2o_model_metrics(h2o_model, h2o_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Table for recall & precision, sensitivity & specificity\n",
    "def h2o_sens_spec(h2o_model, h2o_test, styled=False):\n",
    "    # Predictions\n",
    "    h2o_pred = pd.Series(label_encoder.fit_transform(h2o_model.predict(h2o_test)['predict'].as_data_frame()))\n",
    "    h2o_y = pd.Series(label_encoder.fit_transform(h2o_test['y'].as_data_frame()))\n",
    "\n",
    "    # Recall & Precision values\n",
    "    recall = round(recall_score(h2o_y, h2o_pred, average='macro'), 4)\n",
    "    precision = round(precision_score(h2o_y, h2o_pred, average='macro'), 4)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(h2o_y, h2o_pred)\n",
    "\n",
    "    # List compression for sens & spec values calculation\n",
    "    sensitivity, specificity = zip(*[(round(recall_score(h2o_y, h2o_pred, labels=[i], average='macro'), 4),\n",
    "                                    round(conf_matrix[i, i] / sum(conf_matrix[:, i]), 4))\n",
    "                                    for i in range(len(conf_matrix))])\n",
    "\n",
    "    # Labels and indexes\n",
    "    column_labels = np.unique(h2o_test['y'].as_data_frame())\n",
    "    index_1 = ['Recall:', 'Precision:']\n",
    "    index_2 = [recall, precision]\n",
    "    index_ = [' - ',' - ']\n",
    "    index_3 = ['Sensitivity:', 'Specificity:']\n",
    "\n",
    "    # Build multiindex table\n",
    "    df = pd.DataFrame([sensitivity, specificity])\n",
    "    df.columns = column_labels\n",
    "    df.index = [index_1, index_2, index_, index_3]\n",
    "\n",
    "    # Dataframe style\n",
    "    if styled:\n",
    "        title = f'{h2o_model.key} Model'           \n",
    "        df.columns = pd.MultiIndex.from_tuples([(title, col) for col in df.columns])\n",
    "        return df.style.set_table_styles([{'selector': 'th.col_heading', 'props': 'text-align: center;'}], overwrite=False)\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "# Call above function\n",
    "h2o_sens_spec(h2o_model, h2o_test, styled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation inform\n",
    "h2o_model.explain(h2o_test, exclude_explanations='pdp')\n",
    "\n",
    "# All behind is generated by this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model lists by training framework\n",
    "sklearn_models = ['nb_MA_train', 'GBM_MA_train', 'rf_MA_train', 'nnet_MA_train', 'cart_MA_train']\n",
    "keras_models = ['keras_model_1', 'keras_model_2']\n",
    "h2o_models = ['mod_aml']\n",
    "\n",
    "# Load models by training framework\n",
    "for model in sklearn_models:\n",
    "    globals()[model] = joblib.load(models_folder + '/' + model + '.pkl')\n",
    "    print(f'{model} loaded')\n",
    "for model in keras_models:\n",
    "    globals()[model] = load_model(models_folder + '/' + model + '.keras')\n",
    "    print(f'{model} loaded')\n",
    "for model in h2o_models:\n",
    "    globals()[model] = h2o.load_model(models_folder + '/' + model + '.h2o')\n",
    "    print(f'{model} loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Features importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance comparison chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built dataframe with all feature importance functions along\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'NB': sklearn_theta_feature_importances(nb_MA_train).reset_index()['variable_name'],\n",
    "    'GBM': sklearn_feature_importances(GBM_MA_train).reset_index()['variable_name'],\n",
    "    'RF': sklearn_feature_importances(rf_MA_train).reset_index()['variable_name'],\n",
    "    'NNET': sklearn_coef_feature_importances(nnet_MA_train).reset_index()['variable_name'],\n",
    "    'CART': sklearn_feature_importances(cart_MA_train).reset_index()['variable_name'],\n",
    "    \n",
    "    'keras Sec': keras_sec_importances(keras_model_1).reset_index()['variable_name'],\n",
    "    'keras Func *': keras_func_importances(keras_model_2).reset_index()['variable_name'],\n",
    "\n",
    "    'h2o *': pd.concat([h2o_model.varimp(use_pandas=True), pd.Series([np.nan] * 19)]).reset_index()['variable']\n",
    "    }).head(10).applymap(lambda value: value[-15:])\n",
    "\n",
    "# Invert only index\n",
    "importance_comparison.index = importance_comparison.index + 1\n",
    "importance_comparison.index.name = 'ranking'\n",
    "# Show table\n",
    "importance_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *¬†Remarks:\n",
    "> * In Keras Functional model, variable categories are numbered (_0, _1, _2, etc.).\n",
    "> * In h2o best model, there is no level distinction in the categorical variables.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Performance of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main metrics comparison chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built dataframe with all performance functions along\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'NB': ma_model_metrics(nb_MA_train, X_test, y_test)['value'],\n",
    "    'GBM': ma_model_metrics(GBM_MA_train, X_test, y_test)['value'],\n",
    "    'RF': ma_model_metrics(rf_MA_train, X_test, y_test)['value'],\n",
    "    'NNET': ma_model_metrics(nnet_MA_train, X_test, y_test)['value'],\n",
    "    'CART': ma_model_metrics(cart_MA_train, X_test, y_test)['value'],\n",
    "\n",
    "    'keras Sec': keras_model_metrics(keras_model_1, X, y_ohe)['value'],\n",
    "    'keras Func': keras_model_metrics(keras_model_2, X_joined, y_ohe)['value'],\n",
    "\n",
    "    'h2o': h2o_model_metrics(h2o_model, h2o_test)['value']\n",
    "    }).set_index(ma_model_metrics(nb_MA_train, X, y)['metric'])\n",
    "\n",
    "# Show table\n",
    "importance_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall & Precision comparison chart (Sensitivity & Specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built dataframe with all sens_spec functions along\n",
    "recprec_comparison = pd.DataFrame({\n",
    "    'NB': sens_spec(nb_MA_train, X_test, y_test).index.get_level_values(1),\n",
    "    'GBM':sens_spec(GBM_MA_train, X_test, y_test).index.get_level_values(1),\n",
    "    'RF': sens_spec(rf_MA_train, X_test, y_test).index.get_level_values(1),\n",
    "    'NNET': sens_spec(nnet_MA_train, X_test, y_test).index.get_level_values(1),\n",
    "    'CART': sens_spec(cart_MA_train, X_test, y_test).index.get_level_values(1),\n",
    "\n",
    "    'keras Sec': keras_sens_spec(keras_model_1, X, y_ohe).index.get_level_values(1),\n",
    "    'keras Func': keras_sens_spec(keras_model_2, X_joined, y_ohe).index.get_level_values(1),\n",
    "\n",
    "    'h2o': h2o_sens_spec(h2o_model, h2o_test).index.get_level_values(1)\n",
    "    }).set_index(sens_spec(nb_MA_train, X_test, y_test).index.get_level_values(0))\n",
    "\n",
    "# Show table\n",
    "recprec_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for Cross Validation (10 splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_export_enabled :\n",
    "    # Calculate scores for accuracy for sklearn models\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    accuracy_scores = []\n",
    "    for model in sklearn_models:\n",
    "        accuracy_scores.append(cross_val_score(eval(model), X_train, y_train, scoring='accuracy', cv=kfold, n_jobs=n_jobs))\n",
    "\n",
    "    # Store scores in a pandas dataframe and export to external file\n",
    "    accuracy_scores = pd.DataFrame(accuracy_scores)\n",
    "    accuracy_scores.columns = accuracy_scores.columns.astype(str)\n",
    "    accuracy_scores.reset_index().to_feather(datasets_folder + '/' + 'accuracy_scores_MA.feather')\n",
    "\n",
    "else:\n",
    "    # Load this dataframe otherwise\n",
    "    accuracy_scores = pd.read_feather(datasets_folder + '/' + 'accuracy_scores_MA.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots of accuracy values in cross validation\n",
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "fig.suptitle('Cross Validation Accuracy')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(accuracy_scores[::-1].T.iloc[1:].reset_index(drop=True), vert=False)\n",
    "ax.set_yticklabels([model.split('_')[0].upper() for model in sklearn_models][::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Convert score data to list\n",
    "score_list = accuracy_scores.values.tolist()\n",
    "\n",
    "# Initiate table\n",
    "table=[]\n",
    "for i in range(len(score_list)):\n",
    "    table.append([])\n",
    "for i in range(len(score_list)):\n",
    "    for j in range(len(score_list)):\n",
    "        table[i].append(0)\n",
    "\n",
    "# Populate values   \n",
    "for i in range(len(score_list)): \n",
    "    for j in range(i+1,len(score_list)):\n",
    "        stat, p = ttest_rel(score_list[i], score_list[j])\n",
    "      \n",
    "        alpha = 0.05\n",
    "        \n",
    "        table[i][j]=np.round(p,3) # upper diagonal\n",
    "        table[j][i]=np.round(stat,3) # lower diagonal\n",
    "        table[i][i]=''\n",
    "\n",
    "print('The following table shows the comparison of the statistics and the p-values of the models')\n",
    "print('The value of the statistic on the lower diagonal and the p-value on the upper diagonal')\n",
    "\n",
    "# Show table, including model names\n",
    "table = pd.DataFrame(table).set_index(pd.Index([model.split('_')[0].upper() for model in sklearn_models]), inplace=False)\n",
    "table.columns = [model.split('_')[0].upper() for model in sklearn_models]\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = ['rf_MA_train', 'GBM_MA_train', 'keras_model_1']\n",
    "\n",
    "# Create dalex explainers dict for selected models\n",
    "explainers = {}\n",
    "for model in selected_models:\n",
    "    explainers[model] = dx.Explainer(eval(model), X_train, y_train, label=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Residual Diagnostics\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_diagnostics().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Residual Diagnostics (Absolute)\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_diagnostics().plot(variable = \"ids\", yvariable = \"abs_residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Global Explainability - Variable importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variable importances for selected models\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_parts().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Partial Dependence Profile (PDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDP for selected models\n",
    "for explainer in explainers.values():\n",
    "    explainer.model_profile(variables = [\"vessel_class_Recreational\", \"watertype_river\"] , type = \"partial\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Break Down profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample observation\n",
    "vessel_sample = X_train.sample(n=1, random_state=seed)\n",
    "\n",
    "# Show this observation\n",
    "vessel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shapley values for selected observation\n",
    "for explainer in explainers.values():\n",
    "    explainer.predict_parts(new_observation = vessel_sample, type = \"break_down\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. SHapley Additive exPlanations (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions (time-consuming process)\n",
    "if train_model_enabled :\n",
    "    shaps = {}\n",
    "    for model in explainers:\n",
    "        shaps[model] = explainers[model].predict_parts(new_observation = vessel_sample, type = \"shap\")\n",
    "    joblib.dump(shaps, models_folder + '/' + 'MA_shaps.pkl')\n",
    "else:\n",
    "    shaps = joblib.load(models_folder + '/' + 'MA_shaps.pkl')\n",
    "\n",
    "# Plot shapley values\n",
    "for model in explainers:\n",
    "     shaps[model].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Ceteris Paribus profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shapley values for selected observation\n",
    "for explainer in explainers.values():\n",
    "    explainer.predict_profile(new_observation = vessel_sample).plot(variables = [\"vessel_class_Recreational\", \"watertype_river\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version\n",
    "import sys\n",
    "print(f'Python version: {sys.version}')\n",
    "\n",
    "# Initiate variables for package versions\n",
    "pkgs_dict = {}\n",
    "version = []\n",
    "\n",
    "# Loop importing __version__ for each package (because no whole packages are imported)\n",
    "for pkg in ['pandas', 'numpy', 'sklearn', 'keras', 'tensorflow', 'h2o', 'dalex']:\n",
    "    exec(f\"from {pkg} import __version__ as version\")\n",
    "    pkgs_dict[pkg] = version\n",
    "    \n",
    "# Show as table\n",
    "pd.DataFrame(pkgs_dict, index=[\"Package version\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
